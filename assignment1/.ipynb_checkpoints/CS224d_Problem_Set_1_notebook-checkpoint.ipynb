{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile q1_softmax.py\n",
    "\n",
    "# %load q1_softmax.py\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function for each row of the input x.\n",
    "\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code.\n",
    "    You might find numpy functions np.exp, np.sum, np.reshape,\n",
    "    np.max, and numpy broadcasting useful for this task. (numpy\n",
    "    broadcasting documentation:\n",
    "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
    "\n",
    "    You should also make sure that your code works for one\n",
    "    dimensional inputs (treat the vector as a row), you might find\n",
    "    it helpful for your later problems.\n",
    "\n",
    "    You must implement the optimization in problem 1(a) of the \n",
    "    written assignment!\n",
    "    \"\"\"\n",
    "    nrow = len(x)\n",
    "    \n",
    "    if isinstance(x[0], np.ndarray)==True:\n",
    "        ncol = len(x[0])\n",
    "    else:\n",
    "        #1D array case\n",
    "        c = np.amax(x)\n",
    "        x -= c\n",
    "        x = np.exp(x)\n",
    "        expsum = np.sum(x)\n",
    "        x /= expsum\n",
    "        #x = [i/expsum for i in x]\n",
    "\n",
    "        return x\n",
    "    \n",
    "    #2D array case    \n",
    "    for i in range(nrow):\n",
    "        c = np.amax(x[i,:])\n",
    "        x[i] -= c\n",
    "        \n",
    "    x = np.exp(x)\n",
    "    expsum = np.sum(x,axis=1)\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            x[i,j] /= expsum[i]\n",
    "    return x\n",
    "\n",
    "# def softmax(x):\n",
    "#     \"\"\"\n",
    "#     Compute the softmax function for each row of the input x.\n",
    "\n",
    "#     It is crucial that this function is optimized for speed because\n",
    "#     it will be used frequently in later code.\n",
    "#     You might find numpy functions np.exp, np.sum, np.reshape,\n",
    "#     np.max, and numpy broadcasting useful for this task. (numpy\n",
    "#     broadcasting documentation:\n",
    "#     http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
    "\n",
    "#     You should also make sure that your code works for one\n",
    "#     dimensional inputs (treat the vector as a row), you might find\n",
    "#     it helpful for your later problems.\n",
    "\n",
    "#     You must implement the optimization in problem 1(a) of the\n",
    "#     written assignment!\n",
    "#     \"\"\"\n",
    "#     ### YOUR CODE HERE\n",
    "#     log_c = np.max(x, axis=x.ndim - 1, keepdims=True)\n",
    "#     #for numerical stability\n",
    "#     y = np.sum(np.exp(x - log_c), axis=x.ndim - 1, keepdims=True)\n",
    "#     x = np.exp(x - log_c)/y\n",
    "#     ### END YOUR CODE\n",
    "#     return x\n",
    "\n",
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print (\"Running basic tests...\")\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print (test1)\n",
    "    assert np.amax(np.fabs(test1 - np.array(\n",
    "        [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print (test2)\n",
    "    assert np.amax(np.fabs(test2 - np.array(\n",
    "        [[0.26894142, 0.73105858], [0.26894142, 0.73105858]]))) <= 1e-6\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print (test3)\n",
    "    assert np.amax(np.fabs(test3 - np.array(\n",
    "        [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "    print (\"You should verify these results!\\n\")\n",
    "\n",
    "def test_softmax():\n",
    "    \"\"\" \n",
    "    Use this space to test your softmax implementation by running:\n",
    "        python q1_softmax.py \n",
    "    This function will not be called by the autograder, nor will\n",
    "    your tests be graded.\n",
    "    \"\"\"\n",
    "#     print (\"Running your tests...\")\n",
    "    ### YOUR CODE HERE\n",
    "#     raise NotImplementedError\n",
    "    ### END YOUR CODE  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_softmax_basic()\n",
    "    test_softmax()\n",
    "    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load q2_sigmoid.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load q2_gradcheck.py\n",
    "# %load q2_gradcheck.py\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        ### try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        \n",
    "        old_xix = x[ix]\n",
    "        x[ix] = old_xix + h\n",
    "        random.setstate(rndstate)\n",
    "        fx_pos, grad_pos = f(x)\n",
    "        x[ix] = old_xix - h\n",
    "        random.setstate(rndstate)\n",
    "        fx_neg, grad_neg = f(x)\n",
    "        x[ix] = old_xix\n",
    "\n",
    "        numgrad = (fx_pos - fx_neg)/(2.0*h)\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print (\"Gradient check failed.\")\n",
    "            print (\"First gradient error found at index %s\" % str(ix))\n",
    "            print (\"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad))\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print (\"Gradient check passed!\")\n",
    "\n",
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Some basic sanity checks.\n",
    "    \"\"\"\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print (\"Running sanity checks...\")\n",
    "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
    "    print (\"\")\n",
    "\n",
    "def your_sanity_checks(): \n",
    "    \"\"\"\n",
    "    Use this space add any additional sanity checks by running:\n",
    "        python q2_gradcheck.py \n",
    "    This function will not be called by the autograder, nor will\n",
    "    your additional tests be graded.\n",
    "    \"\"\"\n",
    "#     print \"Running your sanity checks...\"\n",
    "#     ### YOUR CODE HERE\n",
    "#     raise NotImplementedError\n",
    "#     ### END YOUR CODE\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check()\n",
    "    your_sanity_checks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load q2_neural.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile q3_word2vec.py\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    # Implement a function that normalizes each row of a matrix to have unit length\n",
    "    x2 = x*x\n",
    "    x2_sum = np.sum(x2, axis = 1)\n",
    "    rowTotalSum = np.sqrt(x2_sum)\n",
    "    x = np.divide(x,rowTotalSum[:,None])\n",
    "    \n",
    "#     y = np.linalg.norm(x,axis=1,keepdims=True) #<--- simpler method\n",
    "#     x /= y\n",
    "    return x\n",
    "\n",
    "def test_normalize_rows():\n",
    "    print (\"Testing normalizeRows...\")\n",
    "    x = normalizeRows(np.array([[3.0,4.0],[1, 2]])) \n",
    "    # the result should be [[0.6, 0.8], [0.4472, 0.8944]]\n",
    "    print (x)\n",
    "    assert (x.all() == np.array([[0.6, 0.8], [0.4472, 0.8944]]).all())\n",
    "    print (\"\")\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "    \n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, assuming the softmax prediction function and cross      \n",
    "    # entropy loss.                                                   \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - predicted: numpy ndarray, predicted word vector (\\hat{v} in <--- V\n",
    "    #   the written component or \\hat{r} in an earlier version)\n",
    "    # - target: integer, the index of the target word               \n",
    "    # - outputVectors: \"output\" vectors (as rows) for all tokens    <--- U\n",
    "    # - dataset: needed for negative sampling, unused here.         \n",
    "    \n",
    "    # Outputs:                                                        \n",
    "    # - cost: cross entropy cost for the softmax word prediction    \n",
    "    # - gradPred: the gradient with respect to the predicted word   \n",
    "    #        vector                                                \n",
    "    # - grad: the gradient with respect to all the other word        \n",
    "    #        vectors                                               \n",
    "    \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!                                                  \n",
    "    \n",
    "#     ### YOUR CODE HERE: forward propagation\n",
    "#     print(\"predicted shape =\",predicted.shape)\n",
    "#     print(\"outputVectors shape =\",outputVectors.shape)\n",
    "#     Z = np.dot(outputVectors,predicted)\n",
    "#     y_hat = np.array(softmax(Z)) #<--- np.asarray change it from list to array\n",
    "#     cost =-np.log(y_hat[target])\n",
    "\n",
    "#     ### YOUR CODE HERE: backward propagation\n",
    "#     delta_y_hat = y_hat\n",
    "#     delta_y_hat[target] -= 1  \n",
    "#     N = delta_y_hat.shape[0]    #this is the size of |V|,which is the same as Dx and Dy\n",
    "#     H = predicted.shape[0]      #this is the size of hidden layer H\n",
    "#     grad = delta_y_hat.reshape((N,1)) * predicted.reshape((1,H)) #this is dJ/dU\n",
    "#     gradPred = (delta_y_hat.reshape((1,N)).dot(outputVectors)).flatten() #this is dJ/dV\n",
    "    \n",
    "    N, D     = outputVectors.shape\n",
    "\n",
    "    r    = predicted\n",
    "    prob = softmax(r.dot(outputVectors.T))\n",
    "    cost = -np.log(prob[target])\n",
    "\n",
    "    dx   = prob\n",
    "    dx[target] -= 1. #y^hat - y\n",
    "\n",
    "    grad     = dx.reshape((N,1)) * r.reshape((1,D))\n",
    "    gradPred = (dx.reshape((1,N)).dot(outputVectors)).flatten()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset, \n",
    "    K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models \"\"\"\n",
    "\n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, using the negative sampling technique. K is the sample  \n",
    "    # size. You might want to use dataset.sampleTokenIdx() to sample  \n",
    "    # a random word index. \n",
    "    # \n",
    "    # Note: See test_word2vec below for dataset's initialization.\n",
    "    #                                       \n",
    "    # Input/Output Specifications: same as softmaxCostAndGradient     \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### sample out some negative samples\n",
    "    indices = [target]\n",
    "    labels = np.array([1])\n",
    "    for i in range(K):\n",
    "        newIndex = dataset.sampleTokenIdx()\n",
    "        while newIndex == target:\n",
    "            newIndex = dataset.sampleTokenIdx()  # <--- keep on sampling until newIndex != target.\n",
    "        indices.append(newIndex)\n",
    "        labels = np.append(labels,[-1])     # <--- add -1 to labels vector, now we have (1, -1, ..., -1) (-1: k times)\n",
    "        \n",
    "    U = outputVectors[indices,:] # <--- pick out target, and the negative samples' output vector (k*1 x N)\n",
    "    \n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    Z = np.dot(U,predicted)*labels # <--- times 1 if it is target, times -1 if neg sample\n",
    "    y_hat = sigmoid(Z)\n",
    "\n",
    "    J = np.log(y_hat)\n",
    "    cost = -np.sum(J)\n",
    "\n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    gradPred = np.zeros(predicted.shape)   \n",
    "    V = predicted.shape[0] # <--- size of vocab\n",
    "\n",
    "    delta_y_hat = (y_hat-1)*labels    # <--- sigmoid(u*v_c) - 1, times 1 if it is target, -1 if neg sample\n",
    "    \n",
    "    gradPred = np.dot(delta_y_hat.reshape((1,K+1)), U).flatten() # <--- dJ/dVc\n",
    "    \n",
    "    gradTemp = np.dot(delta_y_hat.reshape((K+1,1)), predicted.reshape(1,V)) # <--- dJ/dUk (also target word too)\n",
    "    \n",
    "    for i in range (K+1):\n",
    "        grad[indices[i]] += gradTemp[i,:] # <--- other grad is 0, only update target and negative sample\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n",
    "    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "\n",
    "    # Implement the skip-gram model in this function.\n",
    "\n",
    "    # Inputs:                                                         \n",
    "    # - currrentWord: a string of the current center word           \n",
    "    # - C: integer, context size                                    \n",
    "    # - contextWords: list of no more than 2*C strings, the context words                                               \n",
    "    # - tokens: a dictionary that maps words to their indices in    \n",
    "    #      the word vector list                                \n",
    "    # - inputVectors: \"input\" word vectors (as rows) for all tokens           \n",
    "    # - outputVectors: \"output\" word vectors (as rows) for all tokens         \n",
    "    # - word2vecCostAndGradient: the cost and gradient function for \n",
    "    #      a prediction vector given the target word vectors,  \n",
    "    #      could be one of the two cost functions you          \n",
    "    #      implemented above\n",
    "\n",
    "    # Outputs:                                                        \n",
    "    # - cost: the cost function value for the skip-gram model       \n",
    "    # - grad: the gradient with respect to the word vectors         \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    c_index = tokens[currentWord] #<--- get current word vector\n",
    "    v_c = inputVectors[c_index,:] #<--- get kth row from V\n",
    "    \n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros_like(inputVectors)\n",
    "    gradOut = np.zeros_like(outputVectors)\n",
    "    \n",
    "    for i in contextWords: #<--- loop through window\n",
    "        contextWords_index = tokens[i]\n",
    "        cost_i, gradPred_i, grad_i = word2vecCostAndGradient(v_c, contextWords_index, outputVectors, dataset)\n",
    "        cost += cost_i\n",
    "        gradOut += grad_i\n",
    "        \n",
    "        gradIn[c_index,:] += gradPred_i # <--- this is dJ/dVc, so update Vc only!!! stuck here for a long time\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    \n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n",
    "    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" CBOW model in word2vec \"\"\"\n",
    "\n",
    "    # Implement the continuous bag-of-words model in this function.            \n",
    "    # Input/Output specifications: same as the skip-gram model        \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "\n",
    "    #################################################################\n",
    "    # IMPLEMENTING CBOW IS EXTRA CREDIT, DERIVATIONS IN THE WRIITEN #\n",
    "    # ASSIGNMENT ARE NOT!                                           #  \n",
    "    #################################################################\n",
    "    \n",
    "        # Inputs:                                                         \n",
    "    # - currrentWord: a string of the current center word           \n",
    "    # - C: integer, context size                                    \n",
    "    # - contextWords: list of no more than 2*C strings, the context words                                               \n",
    "    # - tokens: a dictionary that maps words to their indices in    \n",
    "    #      the word vector list                                \n",
    "    # - inputVectors: \"input\" word vectors (as rows) for all tokens           \n",
    "    # - outputVectors: \"output\" word vectors (as rows) for all tokens         \n",
    "    # - word2vecCostAndGradient: the cost and gradient function for \n",
    "    #      a prediction vector given the target word vectors,  \n",
    "    #      could be one of the two cost functions you          \n",
    "    #      implemented above\n",
    "\n",
    "    # Outputs:                                                        \n",
    "    # - cost: the cost function value for the skip-gram model       \n",
    "    # - grad: the gradient with respect to the word vectors         \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    c_index = tokens[currentWord]\n",
    "    one_hot = np.zeros((2*C,len(tokens))) #<--- tokens is a dict, it has no shape! use len\n",
    "    \n",
    "    for i, word in enumerate(contextWords):\n",
    "        one_hot[i, tokens[word]] = 1.      #<--- one_hot array is all the context words one_hot vectors\n",
    "        \n",
    "#     for i in contextWords:\n",
    "#         contextWords_index = tokens[i]\n",
    "#         one_hot[i, contextWords_index] = 1 \n",
    "    \n",
    "    V = np.dot(one_hot, inputVectors) #<--- V is the set of vk, where they are input vectors for context words\n",
    "    \n",
    "    h = (1 / (2*C)) * np.sum(V, axis=0) #<--- why? take average?\n",
    "    \n",
    "    cost, gradPred, gradOut = word2vecCostAndGradient(h, c_index, outputVectors, dataset)\n",
    "    \n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    for i in contextWords:\n",
    "        gradIn[tokens[i]] += (1 / (2*C)) * gradPred\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2,:]\n",
    "    outputVectors = wordVectors[N/2:,:]\n",
    "    for i in range(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        \n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "        \n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N/2, :] += gin / batchsize / denom\n",
    "        grad[N/2:, :] += gout / batchsize / denom\n",
    "        \n",
    "    return cost, grad\n",
    "\n",
    "def test_word2vec():\n",
    "    # Interface to the dataset for negative sampling\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] \\\n",
    "           for i in range(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "    print (\"==== Gradient check for skip-gram ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "    print (\"\\n==== Gradient check for CBOW      ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "\n",
    "    print (\"\\n=== Results ===\")\n",
    "    print (skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset))\n",
    "    print (skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient))\n",
    "    print (cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset))\n",
    "    print (cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_normalize_rows()\n",
    "    test_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile q3_sgd.py\n",
    "# %load q3_sgd.py\n",
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 1000\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import pickle as pickle\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\" A helper function that loads previously saved parameters and resets iteration start \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "            \n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"r\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "    \n",
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"wb\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing = None, useSaved = False, PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent \"\"\"\n",
    "    # Implement the stochastic gradient descent method in this        \n",
    "    # function.                                                       \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - f: the function to optimize, it should take a single        \n",
    "    #     argument and yield two outputs, a cost and the gradient  \n",
    "    #     with respect to the arguments                            \n",
    "    # - x0: the initial point to start SGD from                     \n",
    "    # - step: the step size for SGD                                 \n",
    "    # - iterations: total iterations to run SGD for                 \n",
    "    # - postprocessing: postprocessing function for the parameters  \n",
    "    #     if necessary. In the case of word2vec we will need to    \n",
    "    #     normalize the word vectors to have unit length.          \n",
    "    # - PRINT_EVERY: specifies every how many iterations to output  \n",
    "\n",
    "    # Output:                                                         \n",
    "    # - x: the parameter value after SGD finishes  \n",
    "    \n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "    \n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx;\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "            \n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "    \n",
    "    x = x0\n",
    "    \n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "    \n",
    "    expcost = None\n",
    "    \n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "        ### Don't forget to apply the postprocessing after every iteration!\n",
    "        ### You might want to print the progress every few iterations.\n",
    "\n",
    "        cost = None\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        cost, grad = f(x) # <--- get cost and grad from function\n",
    "        \n",
    "        x -= step * grad  # gradient descent\n",
    "\n",
    "        x = postprocessing(x) #postprocessing: normalize word vector in word2vec        \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not expcost:\n",
    "                expcost = cost\n",
    "            else:\n",
    "                expcost = .95 * expcost + .05 * cost\n",
    "            print (\"iter %d: %f\" % (iter, expcost))\n",
    "        \n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "            \n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sanity_check():\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print (\"Running sanity checks...\")\n",
    "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print (\"test 1 result:\", t1)\n",
    "    assert abs(t1) <= 1e-6\n",
    "\n",
    "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print (\"test 2 result:\", t2)\n",
    "    assert abs(t2) <= 1e-6\n",
    "\n",
    "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print (\"test 3 result:\", t3)\n",
    "    assert abs(t3) <= 1e-6\n",
    "    \n",
    "    print (\"\")\n",
    "\n",
    "def your_sanity_checks(): \n",
    "    \"\"\"\n",
    "    Use this space add any additional sanity checks by running:\n",
    "        python q3_sgd.py \n",
    "    This function will not be called by the autograder, nor will\n",
    "    your additional tests be graded.\n",
    "    \"\"\"\n",
    "    print (\"Running your sanity checks...\")\n",
    "    ### YOUR CODE HERE\n",
    "#     raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check();\n",
    "    your_sanity_checks();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load q3_run\n",
    "import random\n",
    "import numpy as np\n",
    "from cs224d.data_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from q3_word2vec import *\n",
    "from q3_sgd import *\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "wordVectors = np.concatenate(((np.random.rand(nWords, dimVectors) - .5) / \\\n",
    "\tdimVectors, np.zeros((nWords, dimVectors))), axis=0)\n",
    "wordVectors0 = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C, \n",
    "    \tnegSamplingCostAndGradient), \n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "print (\"sanity check: cost at convergence should be around or below 10\")\n",
    "\n",
    "# sum the input and output word vectors\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "\n",
    "# Visualize the word vectors you trained\n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "visualizeWords = [\"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\", \n",
    "\t\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "\t\"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\", \n",
    "\t\"annoying\"]\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2]) \n",
    "\n",
    "for i in range(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i], \n",
    "    \tbbox=dict(facecolor='green', alpha=0.1))\n",
    "    \n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "plt.savefig('q3_word_vectors.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'as a randy film about sexy people in gorgeous places being pushed and pulled ( literally and figuratively ) by desire ... ( sex and lucã\\xada ) makes for an arousing good time .'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-894de3e034b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0msanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-894de3e034b4>\u001b[0m in \u001b[0;36msanity_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mdummy_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetRandomTrainSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mdummy_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSentenceFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"==== Gradient check for softmax regression ====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/Documents/Machine Learning/assignment1/cs224d/data_utils.py\u001b[0m in \u001b[0;36mgetRandomTrainSentence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0msentId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentId\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentId\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcategorify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/Documents/Machine Learning/assignment1/cs224d/data_utils.py\u001b[0m in \u001b[0;36msent_labels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mfull_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-lrb-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-rrb-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0msent_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'as a randy film about sexy people in gorgeous places being pushed and pulled ( literally and figuratively ) by desire ... ( sex and lucã\\xada ) makes for an arousing good time .'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from cs224d.data_utils import *\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q3_sgd import load_saved_params\n",
    "\n",
    "def getSentenceFeature(tokens, wordVectors, sentence):\n",
    "    \"\"\" Obtain the sentence feature for sentiment analysis by averaging its word vectors \"\"\"\n",
    "    # Implement computation for the sentence features given a sentence.                                                       \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - tokens: a dictionary that maps words to their indices in    \n",
    "    #          the word vector list                                \n",
    "    # - wordVectors: word vectors (each row) for all tokens                \n",
    "    # - sentence: a list of words in the sentence of interest \n",
    "\n",
    "    # Output:                                                         \n",
    "    # - sentVector: feature vector for the sentence    \n",
    "    \n",
    "    sentVector = np.zeros((wordVectors.shape[1],))\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    for i in sentence:\n",
    "        sentVector += wordVectors[tokens[i]]\n",
    "    \n",
    "    sentVector /= len(sentence)\n",
    "        \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return sentVector\n",
    "\n",
    "def softmaxRegression(features, labels, weights, regularization = 0.0, nopredictions = False):\n",
    "    \"\"\" Softmax Regression \"\"\"\n",
    "    # Implement softmax regression with weight regularization.        \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - features: feature vectors, each row is a feature vector     \n",
    "    # - labels: labels corresponding to the feature vectors         \n",
    "    # - weights: weights of the regressor                           \n",
    "    # - regularization: L2 regularization constant                  \n",
    "    \n",
    "    # Output:                                                         \n",
    "    # - cost: cost of the regressor                                 \n",
    "    # - grad: gradient of the regressor cost with respect to its    \n",
    "    #        weights                                               \n",
    "    # - pred: label predictions of the regressor (you might find    \n",
    "    #        np.argmax helpful)  \n",
    "    \n",
    "    prob = softmax(features.dot(weights))\n",
    "    if len(features.shape) > 1:\n",
    "        N = features.shape[0]\n",
    "    else:\n",
    "        N = 1\n",
    "    # A vectorized implementation of    1/N * sum(cross_entropy(x_i, y_i)) + 1/2*|w|^2\n",
    "    cost = np.sum(-np.log(prob[range(N), labels])) / N \n",
    "    cost += 0.5 * regularization * np.sum(weights ** 2)\n",
    "    \n",
    "    ### YOUR CODE HERE: compute the gradients and predictions\n",
    "    \n",
    "    pred = np.argmax(prob, axis=1) # why is this taking argmax?\n",
    "    \n",
    "    # delta is the gradient associated with the loss (softmax layer only)\n",
    "    delta = prob\n",
    "    delta[np.arange(N), labels] -= 1  # delta = y^-y\n",
    "    delta /= N\n",
    "    grad = np.dot(features.T, delta)     #backprop the weight just like from before\n",
    "    grad += regularization * weights    #adding the regularization to the gradient    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    if nopredictions:\n",
    "        return cost, grad\n",
    "    else:\n",
    "        return cost, grad, pred\n",
    "\n",
    "def accuracy(y, yhat):\n",
    "    \"\"\" Precision for classifier \"\"\"\n",
    "    assert(y.shape == yhat.shape)\n",
    "    return np.sum(y == yhat) * 100.0 / y.size\n",
    "\n",
    "def softmax_wrapper(features, labels, weights, regularization = 0.0):\n",
    "    cost, grad, _ = softmaxRegression(features, labels, weights, \n",
    "        regularization)\n",
    "    return cost, grad\n",
    "\n",
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Run python q4_softmaxreg.py.\n",
    "    \"\"\"\n",
    "    random.seed(314159)\n",
    "    np.random.seed(265)\n",
    "\n",
    "    dataset = StanfordSentiment()\n",
    "    tokens = dataset.tokens()\n",
    "    nWords = len(tokens)\n",
    "\n",
    "    _, wordVectors0, _ = load_saved_params()\n",
    "    wordVectors = (wordVectors0[:nWords-1,:] + wordVectors0[nWords-1:,:])\n",
    "    dimVectors = wordVectors.shape[1]\n",
    "\n",
    "    dummy_weights = 0.1 * np.random.randn(dimVectors, 5)\n",
    "    dummy_features = np.zeros((10, dimVectors))\n",
    "    dummy_labels = np.zeros((10,), dtype=np.int32)    \n",
    "    for i in range(10):\n",
    "        words, dummy_labels[i] = dataset.getRandomTrainSentence()\n",
    "        dummy_features[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "    print (\"==== Gradient check for softmax regression ====\")\n",
    "    gradcheck_naive(lambda weights: softmaxRegression(dummy_features,\n",
    "        dummy_labels, weights, 1.0, nopredictions = True), dummy_weights)\n",
    "\n",
    "    print (\"\\n=== Results ===\")\n",
    "    print (softmaxRegression(dummy_features, dummy_labels, dummy_weights, 1.0))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'as a randy film about sexy people in gorgeous places being pushed and pulled ( literally and figuratively ) by desire ... ( sex and lucã\\xada ) makes for an arousing good time .'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c41aff543c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Load the train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTrainSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mnTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtrainFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/Documents/Machine Learning/assignment1/cs224d/data_utils.py\u001b[0m in \u001b[0;36mgetTrainSentences\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetTrainSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSplitSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetSplitSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/Documents/Machine Learning/assignment1/cs224d/data_utils.py\u001b[0m in \u001b[0;36mgetSplitSentences\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetSplitSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mds_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msampleTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/Documents/Machine Learning/assignment1/cs224d/data_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetSplitSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mds_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msampleTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/Documents/Machine Learning/assignment1/cs224d/data_utils.py\u001b[0m in \u001b[0;36msent_labels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mfull_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-lrb-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-rrb-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0msent_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'as a randy film about sexy people in gorgeous places being pushed and pulled ( literally and figuratively ) by desire ... ( sex and lucã\\xada ) makes for an arousing good time .'"
     ]
    }
   ],
   "source": [
    "# %load q4_sentiment.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cs224d.data_utils import *\n",
    "\n",
    "from q3_sgd import load_saved_params, sgd\n",
    "from q4_softmaxreg import softmaxRegression, getSentenceFeature, accuracy, softmax_wrapper\n",
    "\n",
    "# Try different regularizations and pick the best!\n",
    "# NOTE: fill in one more \"your code here\" below before running!\n",
    "REGULARIZATION = None   # Assign a list of floats in the block below\n",
    "### YOUR CODE HERE\n",
    "REGULARIZATION = np.logspace(-6,0.1,21)        #need to read into this, not sure how to choose regularization\n",
    "REGULARIZATION = np.hstack([0,REGULARIZATION])\n",
    "### END YOUR CODE\n",
    "\n",
    "# Load the dataset\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# Load the word vectors we trained earlier \n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords-1,:] + wordVectors0[nWords-1:,:])\n",
    "dimVectors = wordVectors.shape[1]\n",
    "\n",
    "# Load the train set\n",
    "trainset = dataset.getTrainSentences()\n",
    "nTrain = len(trainset)\n",
    "trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "for i in xrange(nTrain):\n",
    "    words, trainLabels[i] = trainset[i]\n",
    "    trainFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "# Prepare dev set features\n",
    "devset = dataset.getDevSentences()\n",
    "nDev = len(devset)\n",
    "devFeatures = np.zeros((nDev, dimVectors))\n",
    "devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "for i in xrange(nDev):\n",
    "    words, devLabels[i] = devset[i]\n",
    "    devFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "# Try our regularization parameters\n",
    "results = []\n",
    "for regularization in REGULARIZATION:\n",
    "    random.seed(3141)\n",
    "    np.random.seed(59265)\n",
    "    weights = np.random.randn(dimVectors, 5)\n",
    "    print (\"Training for reg=%f\" % regularization )\n",
    "\n",
    "    # We will do batch optimization\n",
    "    weights = sgd(lambda weights: softmax_wrapper(trainFeatures, trainLabels, \n",
    "        weights, regularization), weights, 3.0, 10000, PRINT_EVERY=100)\n",
    "\n",
    "    # Test on train set\n",
    "    _, _, pred = softmaxRegression(trainFeatures, trainLabels, weights)\n",
    "    trainAccuracy = accuracy(trainLabels, pred)\n",
    "    print (\"Train accuracy (%%): %f\" % trainAccuracy)\n",
    "\n",
    "    # Test on dev set\n",
    "    _, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n",
    "    devAccuracy = accuracy(devLabels, pred)\n",
    "    print (\"Dev accuracy (%%): %f\" % devAccuracy)\n",
    "\n",
    "    # Save the results and weights\n",
    "    results.append({\n",
    "        \"reg\" : regularization, \n",
    "        \"weights\" : weights, \n",
    "        \"train\" : trainAccuracy, \n",
    "        \"dev\" : devAccuracy})\n",
    "\n",
    "# Print the accuracies\n",
    "print (\"\")\n",
    "print (\"=== Recap ===\")\n",
    "print (\"Reg\\t\\tTrain\\t\\tDev\")\n",
    "for result in results:\n",
    "    print (\"%E\\t%f\\t%f\" % (\n",
    "        result[\"reg\"], \n",
    "        result[\"train\"], \n",
    "        result[\"dev\"]))\n",
    "print (\"\")\n",
    "\n",
    "# Pick the best regularization parameters\n",
    "BEST_REGULARIZATION = None\n",
    "BEST_WEIGHTS = None\n",
    "\n",
    "### YOUR CODE HERE \n",
    "sorted_results = sorted(results, key=lambda x: x['dev'],reverse=True) # need to read into this\n",
    "BEST_REGULARIZATION = sorted_results[0]['reg'] \n",
    "BEST_WEIGHTS = sorted_results[0]['weights']\n",
    "### END YOUR CODE\n",
    "\n",
    "# Test your findings on the test set\n",
    "testset = dataset.getTestSentences()\n",
    "nTest = len(testset)\n",
    "testFeatures = np.zeros((nTest, dimVectors))\n",
    "testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "for i in xrange(nTest):\n",
    "    words, testLabels[i] = testset[i]\n",
    "    testFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "_, _, pred = softmaxRegression(testFeatures, testLabels, BEST_WEIGHTS)\n",
    "print (\"Best regularization value: %E\" % BEST_REGULARIZATION)\n",
    "print (\"Test accuracy (%%): %f\" % accuracy(testLabels, pred))\n",
    "\n",
    "# Make a plot of regularization vs accuracy\n",
    "plt.plot(REGULARIZATION, [x[\"train\"] for x in results])\n",
    "plt.plot(REGULARIZATION, [x[\"dev\"] for x in results])\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"regularization\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.savefig(\"q4_reg_v_acc.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
