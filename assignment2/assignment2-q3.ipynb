{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS224d Assignment 2 (2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import math\n",
    "import getpass\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python.ops.seq2seq import sequence_loss\n",
    "from copy import deepcopy\n",
    "\n",
    "import data_utils.utils as du\n",
    "import data_utils.ner as ner\n",
    "from model import Model\n",
    "from model import LanguageModel\n",
    "from utils import data_iterator\n",
    "from utils import calculate_perplexity, get_ptb_dataset, Vocab\n",
    "from utils import ptb_iterator, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_weight_init():\n",
    "    \"\"\"\n",
    "    Returns function that creates random tensor. \n",
    "\n",
    "    The specified function will take in a shape (tuple or 1-d array) and must\n",
    "    return a random tensor of the specified shape and must be drawn from the\n",
    "    Xavier initialization distribution.\n",
    "\n",
    "    Hint: You might find tf.random_uniform useful.\n",
    "    \"\"\"\n",
    "    def _xavier_initializer(shape, **kwargs):\n",
    "        \"\"\"Defines an initializer for the Xavier distribution.\n",
    "\n",
    "        This function will be used as a variable scope initializer.\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/how_tos/variable_scope/index.html#initializers-in-variable-scope\n",
    "\n",
    "        Args:\n",
    "          shape: Tuple or 1-d array that species dimensions of requested tensor.\n",
    "        Returns:\n",
    "          out: tf.Tensor of specified shape sampled from Xavier distribution.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        epsilon = np.sqrt(6.0 / np.sum(shape))\n",
    "        out = tf.random_uniform(dtype=tf.float32, shape=shape, minval=-epsilon, maxval=epsilon)\n",
    "        ### END YOUR CODE\n",
    "        return out\n",
    "    \n",
    "    # Returns defined initializer function.\n",
    "    return _xavier_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 RNN: Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### e) Implement RNN Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    batch_size = 64\n",
    "    embed_size = 50\n",
    "    hidden_size = 100\n",
    "    num_steps = 10\n",
    "    max_epochs = 16\n",
    "    early_stopping = 2\n",
    "    dropout = 0.9\n",
    "    lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNLM_Model(LanguageModel):\n",
    "\n",
    "    def load_data(self, debug=False):\n",
    "        \"\"\"Loads starter word-vectors and train/dev/test data.\"\"\"\n",
    "        self.vocab = Vocab()\n",
    "        self.vocab.construct(get_ptb_dataset('train'))\n",
    "        self.encoded_train = np.array(\n",
    "            [self.vocab.encode(word) for word in get_ptb_dataset('train')],\n",
    "            dtype=np.int32)\n",
    "        self.encoded_valid = np.array(\n",
    "            [self.vocab.encode(word) for word in get_ptb_dataset('valid')],\n",
    "            dtype=np.int32)\n",
    "        self.encoded_test = np.array(\n",
    "            [self.vocab.encode(word) for word in get_ptb_dataset('test')],\n",
    "            dtype=np.int32)\n",
    "        if debug:\n",
    "            num_debug = 1024\n",
    "            self.encoded_train = self.encoded_train[:num_debug]\n",
    "            self.encoded_valid = self.encoded_valid[:num_debug]\n",
    "            self.encoded_test = self.encoded_test[:num_debug]\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generate placeholder variables to represent the input tensors\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "        code and will be fed data during training.  Note that when \"None\" is in a\n",
    "        placeholder's shape, it's flexible\n",
    "\n",
    "        Adds following nodes to the computational graph.\n",
    "        (When None is in a placeholder's shape, it's flexible)\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of shape\n",
    "                           (None, num_steps), type tf.int32\n",
    "        labels_placeholder: Labels placeholder tensor of shape\n",
    "                            (None, num_steps), type tf.float32\n",
    "        dropout_placeholder: Dropout value placeholder (scalar),\n",
    "                             type tf.float32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "\n",
    "          self.input_placeholder\n",
    "          self.labels_placeholder\n",
    "          self.dropout_placeholder\n",
    "\n",
    "        (Don't change the variable names)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        # Note: The second argument shape=(None,) indicates that \n",
    "        # these variables take on a 1-dimensional value of a dynamic \n",
    "        # size. We can use the None value in this case to allow for \n",
    "        # arbitrary batch sizes.\n",
    "        self.input_placeholder = tf.placeholder(dtype=tf.int32,\n",
    "                                                shape=(None, self.config.num_steps),\n",
    "                                                name=\"input\")\n",
    "        self.labels_placeholder = tf.placeholder(dtype=tf.float32,\n",
    "                                                 shape=(None, self.config.num_steps),\n",
    "                                                 name=\"labels\")\n",
    "        self.dropout_placeholder = tf.placeholder(dtype=tf.float32,\n",
    "                                                  name=\"dropout\")\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def add_embedding(self):\n",
    "        \"\"\"Add embedding layer.\n",
    "\n",
    "        Hint: This layer should use the input_placeholder to index into the\n",
    "              embedding.\n",
    "        Hint: You might find tf.nn.embedding_lookup useful.\n",
    "        Hint: You might find tf.split, tf.squeeze useful in constructing tensor inputs\n",
    "        Hint: Check the last slide from the TensorFlow lecture.\n",
    "        Hint: Here are the dimensions of the variables you will need to create:\n",
    "\n",
    "          L: (len(self.vocab), embed_size)\n",
    "\n",
    "        Returns:\n",
    "          inputs: List of length num_steps, each of whose elements should be\n",
    "                  a tensor of shape (batch_size, embed_size).\n",
    "        \"\"\"\n",
    "        # The embedding lookup is currently only implemented for the CPU\n",
    "        with tf.device('/cpu:0'):\n",
    "            ### YOUR CODE HERE\n",
    "            with tf.variable_scope(\"embedding_layer\") as scope:\n",
    "                L = tf.get_variable(name=\"Embedding\", shape=(len(self.vocab), self.config.embed_size))\n",
    "            \n",
    "            embeddings = tf.nn.embedding_lookup(L, self.input_placeholder)\n",
    "            split_embeds = tf.split(embeddings,self.config.num_steps,axis=1)\n",
    "            #split_embeds = tf.split(value=embeddings, num_or_size_splits=self.config.num_steps, axis=1)\n",
    "            inputs = [tf.squeeze(embed, axis=1) for embed in split_embeds]\n",
    "            ### END YOUR CODE\n",
    "            return inputs\n",
    "\n",
    "    def add_projection(self, rnn_outputs):\n",
    "        \"\"\"Adds a projection layer.\n",
    "\n",
    "        The projection layer transforms the hidden representation to a distribution\n",
    "        over the vocabulary.\n",
    "\n",
    "        Hint: Here are the dimensions of the variables you will need to\n",
    "              create \n",
    "\n",
    "              U:   (hidden_size, len(vocab))\n",
    "              b_2: (len(vocab),)\n",
    "\n",
    "        Args:\n",
    "          rnn_outputs: List of length num_steps, each of whose elements should be\n",
    "                       a tensor of shape (batch_size, embed_size).\n",
    "        Returns:\n",
    "          outputs: List of length num_steps, each a tensor of shape\n",
    "                   (batch_size, len(vocab)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        with tf.variable_scope(\"projection_layer\") as scope:\n",
    "            U = tf.get_variable(name=\"U\",\n",
    "                                shape=(self.config.hidden_size, len(self.vocab)),\n",
    "                               initializer=xavier_weight_init())\n",
    "            b_2 = tf.get_variable(name=\"b_2\",\n",
    "                                 shape=(len(self.vocab),),\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        outputs = [tf.matmul(h, U) + b_2 for h in rnn_outputs]\n",
    "        ### END YOUR CODE\n",
    "        return outputs\n",
    "\n",
    "    def add_loss_op(self, output):\n",
    "        \"\"\"Adds loss ops to the computational graph.\n",
    "\n",
    "        Hint: Use tensorflow.python.ops.seq2seq.sequence_loss to implement sequence loss. \n",
    "\n",
    "        Args:\n",
    "          output: A tensor of shape (None, self.vocab)\n",
    "        Returns:\n",
    "          loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        # -> From Steven's\n",
    "        weights = tf.ones([self.config.batch_size , self.config.num_steps])\n",
    "        # reshape labels to a single column batch_size * num_steps\n",
    "#         labels = tf.reshape(self.labels_placeholder, [-1])\n",
    "        \n",
    "        labels = tf.reshape(self.labels_placeholder,[self.config.batch_size * self.config.num_steps, -1])\n",
    "\n",
    "        labels = tf.cast(labels, tf.int32)\n",
    "        \n",
    "        \n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits=output,targets=labels,\n",
    "                                               weights=weights)\n",
    "        \n",
    "#         loss = sequence_loss([output], [labels], [weights])\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See \n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Hint: Use tf.train.AdamOptimizer for this model.\n",
    "              Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "          loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "          train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        with tf.variable_scope(\"adam\"):\n",
    "            train_op = tf.train.AdamOptimizer(self.config.lr).minimize(loss)\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.load_data(debug=False)\n",
    "        #self.load_data(debug=True)\n",
    "        self.add_placeholders()\n",
    "        self.inputs = self.add_embedding()\n",
    "        self.rnn_outputs = self.add_model(self.inputs)\n",
    "        self.outputs = self.add_projection(self.rnn_outputs)\n",
    "\n",
    "        # We want to check how well we correctly predict the next word\n",
    "        # We cast o to float64 as there are numerical issues at hand\n",
    "        # (i.e. sum(output of softmax) = 1.00000298179 and not 1)\n",
    "        self.predictions = [tf.nn.softmax(tf.cast(o, 'float64')) for o in self.outputs]\n",
    "        # Reshape the output into len(vocab) sized chunks - the -1 says as many as\n",
    "        # needed to evenly divide\n",
    "#         output = tf.reshape(tf.concat(1, self.outputs), [-1, len(self.vocab)])\n",
    "        output = self.outputs\n",
    "        output = tf.transpose(output,perm=[1,0,2])\n",
    "\n",
    "        self.calculate_loss = self.add_loss_op(output)\n",
    "        self.train_step = self.add_training_op(self.calculate_loss)\n",
    "\n",
    "\n",
    "    def add_model(self, inputs):\n",
    "        \"\"\"Creates the RNN LM model.\n",
    "\n",
    "        In the space provided below, you need to implement the equations for the\n",
    "        RNN LM model. Note that you may NOT use built in rnn_cell functions from\n",
    "        tensorflow.\n",
    "\n",
    "        Hint: Use a zeros tensor of shape (batch_size, hidden_size) as\n",
    "              initial state for the RNN. Add this to self as instance variable\n",
    "\n",
    "              self.initial_state\n",
    "\n",
    "              (Don't change variable name)\n",
    "        Hint: Add the last RNN output to self as instance variable\n",
    "\n",
    "              self.final_state\n",
    "\n",
    "              (Don't change variable name)\n",
    "        Hint: Make sure to apply dropout to the inputs and the outputs.\n",
    "        Hint: Use a variable scope (e.g. \"RNN\") to define RNN variables.\n",
    "        Hint: Perform an explicit for-loop over inputs. You can use\n",
    "              scope.reuse_variables() to ensure that the weights used at each\n",
    "              iteration (each time-step) are the same. (Make sure you don't call\n",
    "              this for iteration 0 though or nothing will be initialized!)\n",
    "        Hint: Here are the dimensions of the various variables you will need to\n",
    "              create:\n",
    "\n",
    "              H: (hidden_size, hidden_size) \n",
    "              I: (embed_size, hidden_size)\n",
    "              b_1: (hidden_size,)\n",
    "\n",
    "        Args:\n",
    "          inputs: List of length num_steps, each of whose elements should be\n",
    "                  a tensor of shape (batch_size, embed_size).\n",
    "        Returns:\n",
    "          outputs: List of length num_steps, each of whose elements should be\n",
    "                   a tensor of shape (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        self.initial_state = tf.zeros((self.config.batch_size, self.config.hidden_size))\n",
    "\n",
    "        with tf.variable_scope('RNN') as scope:\n",
    "            H = tf.get_variable('H', \n",
    "                                shape=(self.config.hidden_size, self.config.hidden_size),\n",
    "                                initializer=xavier_weight_init())\n",
    "            I = tf.get_variable('I',\n",
    "                                shape=(self.config.embed_size, self.config.hidden_size),\n",
    "                                initializer=xavier_weight_init())\n",
    "            b_1 = tf.get_variable('b_1', shape=(self.config.hidden_size,),\n",
    "                                  initializer=tf.constant_initializer(0.0))\n",
    "            \n",
    "            hidden_state = self.initial_state\n",
    "            rnn_outputs = []\n",
    "            \n",
    "            for t in xrange(self.config.num_steps):\n",
    "                if t > 0:\n",
    "                    scope.reuse_variables()\n",
    "                \n",
    "                input_drop = tf.nn.dropout(inputs[t], self.dropout_placeholder)\n",
    "                \n",
    "                z = tf.matmul(hidden_state, H) + tf.matmul(input_drop, I) + b_1\n",
    "                hidden_state = tf.nn.sigmoid(z)\n",
    "                \n",
    "                output_drop = tf.nn.dropout(hidden_state, self.dropout_placeholder)\n",
    "                rnn_outputs.append(output_drop)\n",
    "            \n",
    "        self.final_state = rnn_outputs[-1]\n",
    "\n",
    "        return rnn_outputs\n",
    "\n",
    "\n",
    "    def run_epoch(self, session, data, train_op=None, verbose=10):\n",
    "        config = self.config\n",
    "        dp = config.dropout\n",
    "        \n",
    "        if not train_op:\n",
    "            train_op = tf.no_op()\n",
    "            dp = 1\n",
    "            \n",
    "        total_steps = sum(1 for x in ptb_iterator(data, config.batch_size, config.num_steps))\n",
    "        total_loss = []\n",
    "        state = self.initial_state.eval()\n",
    "        \n",
    "        for step, (x, y) in enumerate(\n",
    "            ptb_iterator(data, config.batch_size, config.num_steps)):\n",
    "            # We need to pass in the initial state and retrieve the final state to give\n",
    "            # the RNN proper history\n",
    "            feed = {self.input_placeholder: x,\n",
    "                  self.labels_placeholder: y,\n",
    "                  self.initial_state: state,\n",
    "                  self.dropout_placeholder: dp}\n",
    "            loss, state, _ = session.run(\n",
    "              [self.calculate_loss, self.final_state, train_op], feed_dict=feed)\n",
    "            total_loss.append(loss)\n",
    "            \n",
    "            if verbose and step % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : pp = {}'.format(\n",
    "                  step, total_steps, np.exp(np.mean(total_loss))))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "        if verbose:\n",
    "            sys.stdout.write('\\r')\n",
    "            \n",
    "        return np.exp(np.mean(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(session, model, config, starting_text='<eos>',\n",
    "                  stop_length=100, stop_tokens=None, temp=1.0):\n",
    "    \"\"\"Generate text from the model.\n",
    "\n",
    "    Hint: Create a feed-dictionary and use sess.run() to execute the model. Note\n",
    "        that you will need to use model.initial_state as a key to feed_dict\n",
    "    Hint: Fetch model.final_state and model.predictions[-1]. (You set\n",
    "        model.final_state in add_model() and model.predictions is set in\n",
    "        __init__)\n",
    "    Hint: Store the outputs of running the model in local variables state and\n",
    "        y_pred (used in the pre-implemented parts of this function.)\n",
    "\n",
    "    Args:\n",
    "    session: tf.Session() object\n",
    "    model: Object of type RNNLM_Model\n",
    "    config: A Config() object\n",
    "    starting_text: Initial text passed to model.\n",
    "    Returns:\n",
    "    output: List of word idxs\n",
    "    \"\"\"\n",
    "    state = model.initial_state.eval()\n",
    "    # Imagine tokens as a batch size of one, length of len(tokens[0])\n",
    "    tokens = [model.vocab.encode(word) for word in starting_text.split()]\n",
    "    for i in xrange(stop_length):\n",
    "        ### YOUR CODE HERE\n",
    "        feed_dict = {\n",
    "            model.initial_state: state,\n",
    "            model.input_placeholder: [tokens[-1:]],\n",
    "            model.dropout_placeholder: 1.0\n",
    "        }\n",
    "        state, y_pred = session.run([model.final_state, model.predictions[-1]], feed_dict=feed_dict)\n",
    "        ### END YOUR CODE\n",
    "        next_word_idx = sample(y_pred[0], temperature=temp)\n",
    "        tokens.append(next_word_idx)\n",
    "        if stop_tokens and model.vocab.decode(tokens[-1]) in stop_tokens:\n",
    "            break\n",
    "    output = [model.vocab.decode(word_idx) for word_idx in tokens]\n",
    "    return output\n",
    "\n",
    "def generate_sentence(session, model, config, *args, **kwargs):\n",
    "    \"\"\"Convenice to generate a sentence from the model.\"\"\"\n",
    "    return generate_text(session, model, config, *args, stop_tokens=['<eos>'], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_RNNLM():\n",
    "    config = Config()\n",
    "    gen_config = deepcopy(config)\n",
    "    gen_config.batch_size = gen_config.num_steps = 1\n",
    "\n",
    "    # We create the training model and generative model\n",
    "    with tf.variable_scope('RNNLM') as scope:\n",
    "        model = RNNLM_Model(config)\n",
    "        # This instructs gen_model to reuse the same variables as the model above\n",
    "        scope.reuse_variables()\n",
    "        gen_model = RNNLM_Model(gen_config)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        best_val_pp = float('inf')\n",
    "        best_val_epoch = 0\n",
    "\n",
    "        session.run(init)\n",
    "        for epoch in xrange(config.max_epochs):\n",
    "            print 'Epoch {}'.format(epoch)\n",
    "            start = time.time()\n",
    "            ###\n",
    "            train_pp = model.run_epoch(\n",
    "              session, model.encoded_train,\n",
    "              train_op=model.train_step)\n",
    "            valid_pp = model.run_epoch(session, model.encoded_valid)\n",
    "            print 'Training perplexity: {}'.format(train_pp)\n",
    "            print 'Validation perplexity: {}'.format(valid_pp)\n",
    "            if valid_pp < best_val_pp:\n",
    "                best_val_pp = valid_pp\n",
    "                best_val_epoch = epoch\n",
    "                saver.save(session, './ptb_rnnlm.weights')\n",
    "            if epoch - best_val_epoch > config.early_stopping:\n",
    "                break\n",
    "            print 'Total time: {}'.format(time.time() - start)\n",
    "\n",
    "        saver.restore(session, './ptb_rnnlm.weights')\n",
    "        test_pp = model.run_epoch(session, model.encoded_test)\n",
    "        print '=-=' * 5\n",
    "        print 'Test perplexity: {}'.format(test_pp)\n",
    "        print '=-=' * 5\n",
    "        starting_text = 'in palo alto'\n",
    "        while starting_text:\n",
    "            print ' '.join(generate_sentence(\n",
    "              session, gen_model, gen_config, starting_text=starting_text, temp=1.0))\n",
    "            starting_text = raw_input('> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929589.0 total words with 10000 uniques\n",
      "929589.0 total words with 10000 uniques\n",
      "Epoch 0\n",
      "150 / 1452 : pp = 1024.00048828"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b84b0cbe39dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_RNNLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-ce59f85c0737>\u001b[0m in \u001b[0;36mtest_RNNLM\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             train_pp = model.run_epoch(\n\u001b[1;32m     26\u001b[0m               \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoded_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m               train_op=model.train_step)\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mvalid_pp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoded_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m'Training perplexity: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2cea636d24d8>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, session, data, train_op, verbose)\u001b[0m\n\u001b[1;32m    289\u001b[0m                   self.dropout_placeholder: dp}\n\u001b[1;32m    290\u001b[0m             loss, state, _ = session.run(\n\u001b[0;32m--> 291\u001b[0;31m               [self.calculate_loss, self.final_state, train_op], feed_dict=feed)\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_RNNLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
