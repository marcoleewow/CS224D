{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929589.0 total words with 10000 uniques\n",
      "Tensor(\"RNNLM/transpose:0\", shape=(64, 10, 10000), dtype=float32)\n",
      "929589.0 total words with 10000 uniques\n",
      "Tensor(\"RNNLM/transpose_1:0\", shape=(?, 1, 10000), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable RNNLM/RNNLM/Embedding/Adam_2/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-07b694db728f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m     \u001b[0mtest_RNNLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-07b694db728f>\u001b[0m in \u001b[0;36mtest_RNNLM\u001b[0;34m()\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;31m# This instructs gen_model to reuse the same variables as the model above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m     \u001b[0mgen_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNLM_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m   \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-07b694db728f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;31m#    print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_training_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-07b694db728f>\u001b[0m in \u001b[0;36madd_training_op\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m### YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;31m#     train_op = tf.train.GradientDescentOptimizer(self.config.lr).minimize(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[0;32m--> 289\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m   def compute_gradients(self, loss, var_list=None,\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    401\u001b[0m                        ([str(v) for _, _, v in converted_grads_and_vars],))\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/training/adam.pyc\u001b[0m in \u001b[0;36m_create_slots\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# Create slots for the first and second moments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36m_zeros_slot\u001b[0;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m       \u001b[0mnamed_slots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslot_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.pyc\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[0;34m(primary, name, dtype, colocate_with_primary)\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   return create_slot(primary, val, name,\n\u001b[0;32m--> 123\u001b[0;31m                      colocate_with_primary=colocate_with_primary)\n\u001b[0m",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.pyc\u001b[0m in \u001b[0;36mcreate_slot\u001b[0;34m(primary, val, name, colocate_with_primary)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolocate_with_primary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_create_slot_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_create_slot_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.pyc\u001b[0m in \u001b[0;36m_create_slot_var\u001b[0;34m(primary, val, scope)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mcurrent_partitioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0mslot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_partitioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    346\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    655\u001b[0m       raise ValueError(\"Variable %s does not exist, or was not created with \"\n\u001b[1;32m    656\u001b[0m                        \u001b[0;34m\"tf.get_variable(). Did you mean to set reuse=None in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                        \"VarScope?\" % name)\n\u001b[0m\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minitializing_from_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\n",
      "\u001b[0;31mValueError\u001b[0m: Variable RNNLM/RNNLM/Embedding/Adam_2/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?"
     ]
    }
   ],
   "source": [
    "# %load q3_RNNLM_steven.py\n",
    "import getpass\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils import calculate_perplexity, get_ptb_dataset, Vocab\n",
    "from utils import ptb_iterator, sample\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.python.ops.seq2seq import sequence_loss\n",
    "from model import LanguageModel\n",
    "\n",
    "# Let's set the parameters of our model\n",
    "# http://arxiv.org/pdf/1409.2329v4.pdf shows parameters that would achieve near\n",
    "# SotA numbers\n",
    "\n",
    "class Config(object):\n",
    "  \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "  The config class is used to store various hyperparameters and dataset\n",
    "  information parameters. Model objects are passed a Config() object at\n",
    "  instantiation.\n",
    "  \"\"\"\n",
    "  batch_size = 64\n",
    "  embed_size = 50\n",
    "  hidden_size = 100\n",
    "  num_steps = 10\n",
    "  max_epochs = 16\n",
    "  early_stopping = 2\n",
    "  dropout = 0.9\n",
    "  lr = 0.001\n",
    "\n",
    "class RNNLM_Model(LanguageModel):\n",
    "\n",
    "  def load_data(self, debug=False):\n",
    "    \"\"\"Loads starter word-vectors and train/dev/test data.\"\"\"\n",
    "    self.vocab = Vocab()\n",
    "    self.vocab.construct(get_ptb_dataset('train'))\n",
    "    self.encoded_train = np.array(\n",
    "        [self.vocab.encode(word) for word in get_ptb_dataset('train')],\n",
    "        dtype=np.int32)\n",
    "    self.encoded_valid = np.array(\n",
    "        [self.vocab.encode(word) for word in get_ptb_dataset('valid')],\n",
    "        dtype=np.int32)\n",
    "    self.encoded_test = np.array(\n",
    "        [self.vocab.encode(word) for word in get_ptb_dataset('test')],\n",
    "        dtype=np.int32)\n",
    "    if debug:\n",
    "      num_debug = 1024\n",
    "      self.encoded_train = self.encoded_train[:num_debug]\n",
    "      self.encoded_valid = self.encoded_valid[:num_debug]\n",
    "      self.encoded_test = self.encoded_test[:num_debug]\n",
    "\n",
    "  def add_placeholders(self):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors\n",
    "\n",
    "    These placeholders are used as inputs by the rest of the model building\n",
    "    code and will be fed data during training.  Note that when \"None\" is in a\n",
    "    placeholder's shape, it's flexible\n",
    "\n",
    "    Adds following nodes to the computational graph.\n",
    "    (When None is in a placeholder's shape, it's flexible)\n",
    "\n",
    "    input_placeholder: Input placeholder tensor of shape\n",
    "                       (None, num_steps), type tf.int32\n",
    "    labels_placeholder: Labels placeholder tensor of shape\n",
    "                        (None, num_steps), type tf.float32\n",
    "    dropout_placeholder: Dropout value placeholder (scalar),\n",
    "                         type tf.float32\n",
    "\n",
    "    Add these placeholders to self as the instance variables\n",
    "  \n",
    "      self.input_placeholder\n",
    "      self.labels_placeholder\n",
    "      self.dropout_placeholder\n",
    "\n",
    "    (Don't change the variable names)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    self.input_placeholder = tf.placeholder(tf.int32, shape=(None, self.config.num_steps))\n",
    "    self.labels_placeholder = tf.placeholder(tf.float32, shape=(None, self.config.num_steps))\n",
    "    self.dropout_placeholder = tf.placeholder(tf.float32, shape=())\n",
    "    ### END YOUR CODE\n",
    "\n",
    "  \n",
    "  def add_embedding(self):\n",
    "    \"\"\"Add embedding layer.\n",
    "\n",
    "    Hint: This layer should use the input_placeholder to index into the\n",
    "          embedding.\n",
    "    Hint: You might find tf.nn.embedding_lookup useful.\n",
    "    Hint: You might find tf.split, tf.squeeze useful in constructing tensor inputs\n",
    "    Hint: Check the last slide from the TensorFlow lecture.\n",
    "    Hint: Here are the dimensions of the variables you will need to create:\n",
    "\n",
    "      L: (len(self.vocab), embed_size)\n",
    "\n",
    "    Returns:\n",
    "      inputs: List of length num_steps, each of whose elements should be\n",
    "              a tensor of shape (batch_size, embed_size).\n",
    "    \"\"\"\n",
    "    # The embedding lookup is currently only implemented for the CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "      ### YOUR CODE HERE\n",
    "      # with tf.Session():\n",
    "      #   embedding = tf.constant([[1], [2], [3], [4]])\n",
    "      #   print embedding.eval()\n",
    "      #   # [batch_size, num_steps, embed_size]\n",
    "      #   inputs = tf.nn.embedding_lookup(embedding, tf.constant([[1, 2], [2, 2]])).eval()\n",
    "      #   # List([batch_size, 1, embed_size])\n",
    "      #   inputs = tf.split(1, 2, inputs)\n",
    "      #   # List([batch_size, embed_size])\n",
    "      #   inputs = [tf.squeeze(x, [1]) for x in inputs]\n",
    "      #   for x in inputs:\n",
    "      #     print x.eval()\n",
    "      embedding = tf.get_variable('Embedding', [len(self.vocab), self.config.embed_size])\n",
    "      inputs = tf.nn.embedding_lookup(embedding, self.input_placeholder)\n",
    "      \n",
    "      \n",
    "      #split the input into shape (?, self.config.num_steps, self.config.embed_size)\n",
    "      inputs = tf.split(inputs,self.config.num_steps,axis=1)\n",
    "#      inputs = tf.split(1, self.config.num_steps, inputs)\n",
    "      inputs = [tf.squeeze(x, [1]) for x in inputs]\n",
    "      ### END YOUR CODE\n",
    "      return inputs\n",
    "\n",
    "  def add_projection(self, rnn_outputs):\n",
    "    \"\"\"Adds a projection layer.\n",
    "\n",
    "    The projection layer transforms the hidden representation to a distribution\n",
    "    over the vocabulary.\n",
    "\n",
    "    Hint: Here are the dimensions of the variables you will need to\n",
    "          create \n",
    "          \n",
    "          U:   (hidden_size, len(vocab))\n",
    "          b_2: (len(vocab),)\n",
    "\n",
    "    Args:\n",
    "      rnn_outputs: List of length num_steps, each of whose elements should be\n",
    "                   a tensor of shape (batch_size, embed_size).\n",
    "    Returns:\n",
    "      outputs: List of length num_steps, each a tensor of shape\n",
    "               (batch_size, len(vocab)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    with tf.variable_scope('RNN'):\n",
    "      U = tf.get_variable('U', [self.config.hidden_size, len(self.vocab)])\n",
    "      b_2 = tf.get_variable('b_2', [len(self.vocab)])\n",
    "      outputs = []\n",
    "      for h in rnn_outputs:\n",
    "        y = tf.matmul(h, U) + b_2\n",
    "        outputs.append(y)\n",
    "    ### END YOUR CODE\n",
    "    return outputs\n",
    "\n",
    "  def add_loss_op(self, output):\n",
    "    \"\"\"Adds loss ops to the computational graph.\n",
    "\n",
    "    Hint: Use tensorflow.python.ops.seq2seq.sequence_loss to implement sequence loss. \n",
    "\n",
    "    Args:\n",
    "      output: A tensor of shape (None, self.vocab)\n",
    "    Returns:\n",
    "      loss: A 0-d tensor (scalar)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    # labels_placeholder: [batch_size, num_steps]\n",
    "    # output: [batch_size * num_steps, len(vocab)]\n",
    "#    weights = tf.ones([self.config.batch_size * self.config.num_steps])\n",
    "#    # reshape labels to a single column batch_size * num_steps\n",
    "#    labels = tf.reshape(self.labels_placeholder, [-1])\n",
    "#    labels = tf.cast(labels, tf.int32)\n",
    "#    loss = sequence_loss([output], [labels], [weights])\n",
    "#    \n",
    "\n",
    "#######################\n",
    "\n",
    "    # this all_ones_weight is needed for using sequence_loss.\n",
    "    all_ones_weight = tf.ones(shape=[self.config.batch_size,self.config.num_steps])\n",
    "    \n",
    "    reshape_labels = tf.reshape(self.labels_placeholder,[self.config.batch_size * self.config.num_steps, -1])\n",
    "    \n",
    "    #     reshape_labels = tf.reshape(self.labels_placeholder,[-1])\n",
    "    \n",
    "    reshape_labels = tf.cast(reshape_labels,tf.int32)\n",
    "    \n",
    "    output_tensor = tf.convert_to_tensor(output)\n",
    "    print(output_tensor)\n",
    "    ## [batch_size x sequence_length x num_decoder_symbols]\n",
    "    CE_loss = tf.contrib.seq2seq.sequence_loss(logits=output_tensor,targets=reshape_labels,\n",
    "                                               weights=all_ones_weight)\n",
    "        \n",
    "    # We need to add ALL the total_loss, so we have to add it to collection and sum them up later.\n",
    "    tf.add_to_collection(\"total_loss\", CE_loss)\n",
    "    loss=tf.add_n(tf.get_collection('total_loss'))\n",
    "       \n",
    "    ### END YOUR CODE\n",
    "    return loss\n",
    "\n",
    "  def add_training_op(self, loss):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "\n",
    "    Creates an optimizer and applies the gradients to all trainable variables.\n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train. See \n",
    "\n",
    "    https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "    for more information.\n",
    "\n",
    "    Hint: Use tf.train.AdamOptimizer for this model.\n",
    "          Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "    Args:\n",
    "      loss: Loss tensor, from cross_entropy_loss.\n",
    "    Returns:\n",
    "      train_op: The Op for training.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    train_op = tf.train.AdamOptimizer(self.config.lr).minimize(loss)\n",
    "#     train_op = tf.train.GradientDescentOptimizer(self.config.lr).minimize(loss)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    return train_op\n",
    "  \n",
    "  def __init__(self, config):\n",
    "    self.config = config\n",
    "    self.load_data(debug=False)\n",
    "    self.add_placeholders()\n",
    "    self.inputs = self.add_embedding()\n",
    "    self.rnn_outputs = self.add_model(self.inputs)\n",
    "    self.outputs = self.add_projection(self.rnn_outputs)\n",
    "  \n",
    "    # We want to check how well we correctly predict the next word\n",
    "    # We cast o to float64 as there are numerical issues at hand\n",
    "    # (i.e. sum(output of softmax) = 1.00000298179 and not 1)\n",
    "    self.predictions = [tf.nn.softmax(tf.cast(o, 'float64')) for o in self.outputs]\n",
    "    # Reshape the output into len(vocab) sized chunks - the -1 says as many as\n",
    "    # needed to evenly divide\n",
    "#    output = tf.reshape(tf.concat(1, self.outputs), [-1, len(self.vocab)])\n",
    "#    output = tf.concat(self.outputs, axis=1)\n",
    "    output = self.outputs\n",
    "    output = tf.transpose(output,perm=[1,0,2])\n",
    "#    print(output)\n",
    "    self.calculate_loss = self.add_loss_op(output)\n",
    "    self.train_step = self.add_training_op(self.calculate_loss)\n",
    "\n",
    "\n",
    "  def add_model(self, inputs):\n",
    "    \"\"\"Creates the RNN LM model.\n",
    "\n",
    "    In the space provided below, you need to implement the equations for the\n",
    "    RNN LM model. Note that you may NOT use built in rnn_cell functions from\n",
    "    tensorflow.\n",
    "\n",
    "    Hint: Use a zeros tensor of shape (batch_size, hidden_size) as\n",
    "          initial state for the RNN. Add this to self as instance variable\n",
    "\n",
    "          self.initial_state\n",
    "  \n",
    "          (Don't change variable name)\n",
    "    Hint: Add the last RNN output to self as instance variable\n",
    "\n",
    "          self.final_state\n",
    "\n",
    "          (Don't change variable name)\n",
    "    Hint: Make sure to apply dropout to the inputs and the outputs.\n",
    "    Hint: Use a variable scope (e.g. \"RNN\") to define RNN variables.\n",
    "    Hint: Perform an explicit for-loop over inputs. You can use\n",
    "          scope.reuse_variables() to ensure that the weights used at each\n",
    "          iteration (each time-step) are the same. (Make sure you don't call\n",
    "          this for iteration 0 though or nothing will be initialized!)\n",
    "    Hint: Here are the dimensions of the various variables you will need to\n",
    "          create:\n",
    "      \n",
    "          H: (hidden_size, hidden_size) \n",
    "          I: (embed_size, hidden_size)\n",
    "          b_1: (hidden_size,)\n",
    "\n",
    "    Args:\n",
    "      inputs: List of length num_steps, each of whose elements should be\n",
    "              a tensor of shape (batch_size, embed_size).\n",
    "    Returns:\n",
    "      outputs: List of length num_steps, each of whose elements should be\n",
    "               a tensor of shape (batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    # with tf.Session():\n",
    "    #   batch_size = 2\n",
    "    #   embed_size = 6\n",
    "    #   num_steps = 5\n",
    "    #   hidden_size = 3\n",
    "    #   dropout = 0.5\n",
    "    #   test_inputs = [tf.random_normal([batch_size, embed_size]) for i in range(num_steps)]\n",
    "    #   test_inputs = [tf.nn.dropout(x, dropout) for x in test_inputs]\n",
    "    #   initial_state = tf.zeros([batch_size, hidden_size])\n",
    "    #   H = tf.random_normal([hidden_size, hidden_size])\n",
    "    #   I = tf.random_normal([embed_size, hidden_size])\n",
    "    #   b_1 = tf.random_normal([hidden_size])\n",
    "    #   h = initial_state\n",
    "    #   rnn_outputs = []\n",
    "    #   for input in test_inputs:\n",
    "    #     h = tf.nn.sigmoid(tf.matmul(h, H) + tf.matmul(input, I) + b_1)\n",
    "    #     rnn_outputs.append(h)\n",
    "    #   rnn_outputs = [tf.nn.dropout(x, dropout) for x in test_inputs]\n",
    "    #   for output in rnn_outputs:\n",
    "    #     print output.eval()\n",
    "\n",
    "    with tf.variable_scope('RNN'):\n",
    "      inputs = [tf.nn.dropout(step, self.dropout_placeholder) for step in inputs]\n",
    "      self.initial_state = tf.zeros([self.config.batch_size, self.config.hidden_size])\n",
    "      H = tf.get_variable('H', [self.config.hidden_size, self.config.hidden_size])\n",
    "      I = tf.get_variable('I', [self.config.embed_size, self.config.hidden_size])\n",
    "      b_1 = tf.get_variable('b_1', [self.config.hidden_size])\n",
    "      h = self.initial_state\n",
    "      rnn_outputs = []\n",
    "      for step in inputs:\n",
    "        h = tf.nn.sigmoid(tf.matmul(h, H) + tf.matmul(step, I) + b_1)\n",
    "        rnn_outputs.append(h)\n",
    "      self.final_state = h\n",
    "      rnn_outputs = [tf.nn.dropout(step, self.dropout_placeholder) for step in rnn_outputs]\n",
    "    ### END YOUR CODE\n",
    "    return rnn_outputs\n",
    "\n",
    "\n",
    "  def run_epoch(self, session, data, train_op=None, verbose=10):\n",
    "    config = self.config\n",
    "    dp = config.dropout\n",
    "    if not train_op:\n",
    "      train_op = tf.no_op()\n",
    "      dp = 1\n",
    "    total_steps = sum(1 for x in ptb_iterator(data, config.batch_size, config.num_steps))\n",
    "    total_loss = []\n",
    "    state = self.initial_state.eval()\n",
    "    for step, (x, y) in enumerate(\n",
    "      ptb_iterator(data, config.batch_size, config.num_steps)):\n",
    "      # We need to pass in the initial state and retrieve the final state to give\n",
    "      # the RNN proper history\n",
    "      feed = {self.input_placeholder: x,\n",
    "              self.labels_placeholder: y,\n",
    "              self.initial_state: state,\n",
    "              self.dropout_placeholder: dp}\n",
    "      loss, state, _ = session.run(\n",
    "          [self.calculate_loss, self.final_state, train_op], feed_dict=feed)\n",
    "      total_loss.append(loss)\n",
    "      if verbose and step % verbose == 0:\n",
    "          sys.stdout.write('\\r{} / {} : pp = {}'.format(\n",
    "              step, total_steps, np.exp(np.mean(total_loss))))\n",
    "          sys.stdout.flush()\n",
    "    if verbose:\n",
    "      sys.stdout.write('\\r')\n",
    "    return np.exp(np.mean(total_loss))\n",
    "\n",
    "def generate_text(session, model, config, starting_text='<eos>',\n",
    "                  stop_length=100, stop_tokens=None, temp=1.0):\n",
    "  \"\"\"Generate text from the model.\n",
    "\n",
    "  Hint: Create a feed-dictionary and use sess.run() to execute the model. Note\n",
    "        that you will need to use model.initial_state as a key to feed_dict\n",
    "  Hint: Fetch model.final_state and model.predictions[-1]. (You set\n",
    "        model.final_state in add_model() and model.predictions is set in\n",
    "        __init__)\n",
    "  Hint: Store the outputs of running the model in local variables state and\n",
    "        y_pred (used in the pre-implemented parts of this function.)\n",
    "\n",
    "  Args:\n",
    "    session: tf.Session() object\n",
    "    model: Object of type RNNLM_Model\n",
    "    config: A Config() object\n",
    "    starting_text: Initial text passed to model.\n",
    "  Returns:\n",
    "    output: List of word idxs\n",
    "  \"\"\"\n",
    "  state = model.initial_state.eval()\n",
    "  # Imagine tokens as a batch size of one, length of len(tokens[0])\n",
    "  tokens = [model.vocab.encode(word) for word in starting_text.split()]\n",
    "  for i in xrange(stop_length):\n",
    "    ### YOUR CODE HERE\n",
    "    feed = {model.input_placeholder: [tokens[-1:]],\n",
    "            model.initial_state: state,\n",
    "            model.dropout_placeholder: 1}\n",
    "    state, y_pred = session.run(\n",
    "        [model.final_state, model.predictions[-1]], feed_dict=feed)\n",
    "    ### END YOUR CODE\n",
    "    next_word_idx = sample(y_pred[0], temperature=temp)\n",
    "    tokens.append(next_word_idx)\n",
    "    if stop_tokens and model.vocab.decode(tokens[-1]) in stop_tokens:\n",
    "      break\n",
    "  output = [model.vocab.decode(word_idx) for word_idx in tokens]\n",
    "  return output\n",
    "\n",
    "def generate_sentence(session, model, config, *args, **kwargs):\n",
    "  \"\"\"Convenice to generate a sentence from the model.\"\"\"\n",
    "  return generate_text(session, model, config, *args, stop_tokens=['<eos>'], **kwargs)\n",
    "\n",
    "def test_RNNLM():\n",
    "  config = Config()\n",
    "  gen_config = deepcopy(config)\n",
    "  gen_config.batch_size = gen_config.num_steps = 1\n",
    "\n",
    "  # We create the training model and generative model\n",
    "  with tf.variable_scope('RNNLM') as scope:\n",
    "    model = RNNLM_Model(config)\n",
    "    # This instructs gen_model to reuse the same variables as the model above\n",
    "    scope.reuse_variables()\n",
    "    gen_model = RNNLM_Model(gen_config)\n",
    "\n",
    "  init = tf.initialize_all_variables()\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  with tf.Session() as session:\n",
    "    best_val_pp = float('inf')\n",
    "    best_val_epoch = 0\n",
    "  \n",
    "    session.run(init)\n",
    "    for epoch in xrange(config.max_epochs):\n",
    "       print 'Epoch {}'.format(epoch)\n",
    "       start = time.time()\n",
    "       ###\n",
    "       train_pp = model.run_epoch(\n",
    "           session, model.encoded_train,\n",
    "           train_op=model.train_step)\n",
    "       valid_pp = model.run_epoch(session, model.encoded_valid)\n",
    "       print 'Training perplexity: {}'.format(train_pp)\n",
    "       print 'Validation perplexity: {}'.format(valid_pp)\n",
    "       if valid_pp < best_val_pp:\n",
    "         best_val_pp = valid_pp\n",
    "         best_val_epoch = epoch\n",
    "         saver.save(session, './ptb_rnnlm.weights')\n",
    "       if epoch - best_val_epoch > config.early_stopping:\n",
    "         break\n",
    "       print 'Total time: {}'.format(time.time() - start)\n",
    "\n",
    "    saver.restore(session, './ptb_rnnlm.weights')\n",
    "    test_pp = model.run_epoch(session, model.encoded_test)\n",
    "    print '=-=' * 5\n",
    "    print 'Test perplexity: {}'.format(test_pp)\n",
    "    print '=-=' * 5\n",
    "    starting_text = 'in palo alto'\n",
    "    while starting_text:\n",
    "      print ' '.join(generate_sentence(\n",
    "          session, gen_model, gen_config, starting_text=starting_text, temp=1.0))\n",
    "      starting_text = raw_input('> ')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_RNNLM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
