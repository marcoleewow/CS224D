{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "test1 =  [[ 0.26894143  0.7310586 ]\n",
      " [ 0.26894143  0.7310586 ]]\n",
      "test1 shape = (2, 2)\n",
      "test2 =  [[ 0.7310586   0.26894143]]\n",
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n",
      "Basic (non-exhaustive) cross-entropy tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%writefile q1_softmax.py\n",
    "# %load q1_softmax.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def softmax(x):\n",
    "  \"\"\"\n",
    "  Compute the softmax function in tensorflow.\n",
    "\n",
    "  You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "  tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "  not need to use all of these functions). Recall also that many common\n",
    "  tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "  if x and y are both tensors). Make sure to implement the numerical stability\n",
    "  fixes as in the previous homework!\n",
    "\n",
    "  Args:\n",
    "    x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "         represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "         input as in the previous homework)\n",
    "  Returns:\n",
    "    out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "         tensor in this problem.\n",
    "  \"\"\"\n",
    "\n",
    "  ### YOUR CODE HERE\n",
    "  max_x = tf.reduce_max(x, axis = 1, keep_dims = True) # Need to use keep_dims for keeping the shape (n,) to (n,1)\n",
    "  denom = tf.reduce_sum(tf.exp(x - max_x), axis = 1, keep_dims=True) \n",
    "  out = tf.exp(x - max_x)/denom\n",
    "  ### END YOUR CODE\n",
    "  \n",
    "  return out \n",
    "\n",
    "def cross_entropy_loss(y, yhat):\n",
    "  \"\"\"\n",
    "  Compute the cross entropy loss in tensorflow.\n",
    "\n",
    "  y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "  of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "  be of dtype tf.float32.\n",
    "\n",
    "  The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "  solutions are possible, so you may not need to use all of these functions).\n",
    "\n",
    "  Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "        functions.\n",
    "\n",
    "  Args:\n",
    "    y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "    yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "          probability distribution and should sum to 1.\n",
    "  Returns:\n",
    "    out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "          tensor in the problem.\n",
    "  \"\"\"\n",
    "  ### YOUR CODE HERE\n",
    "  y_float = tf.to_float(y)\n",
    "\n",
    "  # no need to add axis = 1, because we sum up all the rows then we will also sum up the columns\n",
    "  out = -tf.reduce_sum(tf.multiply(y_float,tf.log(yhat))) \n",
    "  ### END YOUR CODE\n",
    "  return out\n",
    "\n",
    "\n",
    "def test_softmax_basic():\n",
    "  \"\"\"\n",
    "  Some simple tests to get you started. \n",
    "  Warning: these are not exhaustive.\n",
    "  \"\"\"\n",
    "  print (\"Running basic tests...\")\n",
    "  test1 = softmax(tf.convert_to_tensor(\n",
    "      np.array([[1001,1002],[3,4]]), dtype=tf.float32))\n",
    "  with tf.Session():\n",
    "      test1 = test1.eval()\n",
    "      print(\"test1 = \",test1)\n",
    "      print(\"test1 shape =\", test1.shape)\n",
    "  assert np.amax(np.fabs(test1 - np.array(\n",
    "      [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "  test2 = softmax(tf.convert_to_tensor(\n",
    "      np.array([[-1001,-1002]]), dtype=tf.float32))\n",
    "  with tf.Session():\n",
    "      test2 = test2.eval()\n",
    "      print(\"test2 = \",test2)\n",
    "\n",
    "  assert np.amax(np.fabs(test2 - np.array(\n",
    "      [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "  print (\"Basic (non-exhaustive) softmax tests pass\\n\")\n",
    "\n",
    "def test_cross_entropy_loss_basic():\n",
    "  \"\"\"\n",
    "  Some simple tests to get you started.\n",
    "  Warning: these are not exhaustive.\n",
    "  \"\"\"\n",
    "  y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "  yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "  test1 = cross_entropy_loss(\n",
    "      tf.convert_to_tensor(y, dtype=tf.int32),\n",
    "      tf.convert_to_tensor(yhat, dtype=tf.float32))\n",
    "  with tf.Session():\n",
    "    test1 = test1.eval()\n",
    "  result = -3 * np.log(.5)\n",
    "  assert np.amax(np.fabs(test1 - result)) <= 1e-6\n",
    "  print (\"Basic (non-exhaustive) cross-entropy tests pass\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  test_softmax_basic()\n",
    "  test_cross_entropy_loss_basic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 64.27 (0.017 sec)\n",
      "Epoch 1: loss = 22.65 (0.010 sec)\n",
      "Epoch 2: loss = 12.21 (0.010 sec)\n",
      "Epoch 3: loss = 8.16 (0.011 sec)\n",
      "Epoch 4: loss = 6.08 (0.010 sec)\n",
      "Epoch 5: loss = 4.83 (0.010 sec)\n",
      "Epoch 6: loss = 3.99 (0.010 sec)\n",
      "Epoch 7: loss = 3.40 (0.010 sec)\n",
      "Epoch 8: loss = 2.96 (0.010 sec)\n",
      "Epoch 9: loss = 2.62 (0.010 sec)\n",
      "Epoch 10: loss = 2.35 (0.012 sec)\n",
      "Epoch 11: loss = 2.13 (0.011 sec)\n",
      "Epoch 12: loss = 1.94 (0.011 sec)\n",
      "Epoch 13: loss = 1.79 (0.010 sec)\n",
      "Epoch 14: loss = 1.66 (0.010 sec)\n",
      "Epoch 15: loss = 1.54 (0.010 sec)\n",
      "Epoch 16: loss = 1.45 (0.013 sec)\n",
      "Epoch 17: loss = 1.36 (0.011 sec)\n",
      "Epoch 18: loss = 1.28 (0.010 sec)\n",
      "Epoch 19: loss = 1.21 (0.012 sec)\n",
      "Epoch 20: loss = 1.15 (0.017 sec)\n",
      "Epoch 21: loss = 1.09 (0.025 sec)\n",
      "Epoch 22: loss = 1.04 (0.018 sec)\n",
      "Epoch 23: loss = 1.00 (0.012 sec)\n",
      "Epoch 24: loss = 0.95 (0.011 sec)\n",
      "Epoch 25: loss = 0.91 (0.011 sec)\n",
      "Epoch 26: loss = 0.88 (0.011 sec)\n",
      "Epoch 27: loss = 0.85 (0.012 sec)\n",
      "Epoch 28: loss = 0.81 (0.011 sec)\n",
      "Epoch 29: loss = 0.79 (0.012 sec)\n",
      "Epoch 30: loss = 0.76 (0.012 sec)\n",
      "Epoch 31: loss = 0.73 (0.011 sec)\n",
      "Epoch 32: loss = 0.71 (0.010 sec)\n",
      "Epoch 33: loss = 0.69 (0.011 sec)\n",
      "Epoch 34: loss = 0.67 (0.012 sec)\n",
      "Epoch 35: loss = 0.65 (0.012 sec)\n",
      "Epoch 36: loss = 0.63 (0.017 sec)\n",
      "Epoch 37: loss = 0.61 (0.012 sec)\n",
      "Epoch 38: loss = 0.60 (0.012 sec)\n",
      "Epoch 39: loss = 0.58 (0.012 sec)\n",
      "Epoch 40: loss = 0.57 (0.010 sec)\n",
      "Epoch 41: loss = 0.55 (0.010 sec)\n",
      "Epoch 42: loss = 0.54 (0.010 sec)\n",
      "Epoch 43: loss = 0.53 (0.011 sec)\n",
      "Epoch 44: loss = 0.51 (0.014 sec)\n",
      "Epoch 45: loss = 0.50 (0.010 sec)\n",
      "Epoch 46: loss = 0.49 (0.010 sec)\n",
      "Epoch 47: loss = 0.48 (0.010 sec)\n",
      "Epoch 48: loss = 0.47 (0.010 sec)\n",
      "Epoch 49: loss = 0.46 (0.010 sec)\n",
      "Basic (non-exhaustive) classifier tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%writefile q1_classifier.py\n",
    "# %load q1_classifier.py\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from q1_softmax import softmax\n",
    "from q1_softmax import cross_entropy_loss\n",
    "from model import Model\n",
    "from utils import data_iterator\n",
    "\n",
    "class Config(object):\n",
    "  \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "  The config class is used to store various hyperparameters and dataset\n",
    "  information parameters. Model objects are passed a Config() object at\n",
    "  instantiation.\n",
    "  \"\"\"\n",
    "  batch_size = 64\n",
    "  n_samples = 1024\n",
    "  n_features = 100\n",
    "  n_classes = 5\n",
    "  # You may adjust the max_epochs to ensure convergence.\n",
    "  max_epochs = 50\n",
    "  # You may adjust this learning rate to ensure convergence.\n",
    "  lr = 1e-4 \n",
    "\n",
    "class SoftmaxModel(Model):\n",
    "  \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
    "\n",
    "  def load_data(self):\n",
    "    \"\"\"Creates a synthetic dataset and stores it in memory.\"\"\"\n",
    "    np.random.seed(1234)\n",
    "    self.input_data = np.random.rand(\n",
    "        self.config.n_samples, self.config.n_features)\n",
    "    self.input_labels = np.ones((self.config.n_samples,), dtype=np.int32)\n",
    "\n",
    "  def add_placeholders(self):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "\n",
    "    These placeholders are used as inputs by the rest of the model building\n",
    "    code and will be fed data during training.\n",
    "\n",
    "    Adds following nodes to the computational graph\n",
    "\n",
    "    input_placeholder: Input placeholder tensor of shape\n",
    "                       (batch_size, n_features), type tf.float32\n",
    "    labels_placeholder: Labels placeholder tensor of shape\n",
    "                       (batch_size, n_classes), type tf.int32\n",
    "\n",
    "    Add these placeholders to self as the instance variables\n",
    "  \n",
    "      self.input_placeholder\n",
    "      self.labels_placeholder\n",
    "\n",
    "    (Don't change the variable names)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    self.input_placeholder = tf.placeholder(tf.float32, shape = (self.config.batch_size, self.config.n_features))\n",
    "    self.labels_placeholder = tf.placeholder(tf.int32, shape = (self.config.batch_size, self.config.n_classes))\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "  def create_feed_dict(self, input_batch, label_batch):\n",
    "    \"\"\"Creates the feed_dict for softmax classifier.\n",
    "\n",
    "    A feed_dict takes the form of:\n",
    "\n",
    "    feed_dict = {\n",
    "        <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "        ....\n",
    "    }\n",
    "\n",
    "    If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "    Hint: The keys for the feed_dict should match the placeholder tensors\n",
    "          created in add_placeholders.\n",
    "    \n",
    "    Args:\n",
    "      input_batch: A batch of input data.\n",
    "      label_batch: A batch of label data.\n",
    "    Returns:\n",
    "      feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    feed_dict = {\n",
    "        self.input_placeholder:input_batch,\n",
    "#         self.labels_placeholder:label_batch,\n",
    "    }\n",
    "    if label_batch is not None:\n",
    "      feed_dict[self.labels_placeholder] = label_batch\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    return feed_dict\n",
    "\n",
    "  def add_training_op(self, loss):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "\n",
    "    Creates an optimizer and applies the gradients to all trainable variables.\n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train. See \n",
    "\n",
    "    https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "    for more information.\n",
    "\n",
    "    Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "          Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "    Args:\n",
    "      loss: Loss tensor, from cross_entropy_loss.\n",
    "    Returns:\n",
    "      train_op: The Op for training.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    train_op = tf.train.GradientDescentOptimizer(self.config.lr).minimize(loss)\n",
    "    ### END YOUR CODE\n",
    "    return train_op\n",
    "\n",
    "  def add_model(self, input_data):\n",
    "    \"\"\"Adds a linear-layer plus a softmax transformation\n",
    "\n",
    "    The core transformation for this model which transforms a batch of input\n",
    "    data into a batch of predictions. In this case, the mathematical\n",
    "    transformation effected is\n",
    "\n",
    "    y = softmax(xW + b)\n",
    "\n",
    "    Hint: Make sure to create tf.Variables as needed. Also, make sure to use\n",
    "          tf.name_scope to ensure that your name spaces are clean.\n",
    "    Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "          and biases b with zeros.\n",
    "\n",
    "    Args:\n",
    "      input_data: A tensor of shape (batch_size, n_features).\n",
    "    Returns:\n",
    "      out: A tensor of shape (batch_size, n_classes)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        W = tf.Variable(tf.zeros([self.config.n_features,self.config.n_classes]), name = \"Weight\")\n",
    "        b = tf.Variable(tf.zeros([self.config.batch_size,self.config.n_classes]), name = \"Bias\")\n",
    "        mult = tf.matmul(self.input_placeholder,W) ##########\n",
    "#         mult = tf.matmul(input_data,W) ##########\n",
    "\n",
    "        add = tf.add(mult, b)\n",
    "        out = softmax(add)\n",
    "    ### END YOUR CODE\n",
    "        return out\n",
    "\n",
    "  def add_loss_op(self, pred):\n",
    "    \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "    Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "          short function.\n",
    "    Args:\n",
    "      pred: A tensor of shape (batch_size, n_classes)\n",
    "    Returns:\n",
    "      loss: A 0-d tensor (scalar)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    loss = cross_entropy_loss(self.labels_placeholder, pred)\n",
    "    ### END YOUR CODE\n",
    "    return loss\n",
    "\n",
    "  def run_epoch(self, sess, input_data, input_labels):\n",
    "    \"\"\"Runs an epoch of training.\n",
    "\n",
    "    Trains the model for one-epoch.\n",
    "  \n",
    "    Args:\n",
    "      sess: tf.Session() object\n",
    "      input_data: np.ndarray of shape (n_samples, n_features)\n",
    "      input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "    Returns:\n",
    "      average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "    \"\"\"\n",
    "    # And then after everything is built, start the training loop.\n",
    "    average_loss = 0\n",
    "    for step, (input_batch, label_batch) in enumerate(\n",
    "        data_iterator(input_data, input_labels,\n",
    "                      batch_size=self.config.batch_size,\n",
    "                      label_size=self.config.n_classes)):\n",
    "\n",
    "      # Fill a feed dictionary with the actual set of images and labels\n",
    "      # for this particular training step.\n",
    "      feed_dict = self.create_feed_dict(input_batch, label_batch)\n",
    "\n",
    "      # Run one step of the model.  The return values are the activations\n",
    "      # from the `self.train_op` (which is discarded) and the `loss` Op.  To\n",
    "      # inspect the values of your Ops or variables, you may include them\n",
    "      # in the list passed to sess.run() and the value tensors will be\n",
    "      # returned in the tuple from the call.\n",
    "      _, loss_value = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "      average_loss += loss_value\n",
    "\n",
    "    average_loss = average_loss / step\n",
    "    return average_loss \n",
    "\n",
    "  def fit(self, sess, input_data, input_labels):\n",
    "    \"\"\"Fit model on provided data.\n",
    "\n",
    "    Args:\n",
    "      sess: tf.Session()\n",
    "      input_data: np.ndarray of shape (n_samples, n_features)\n",
    "      input_labels: np.ndarray of shape (n_samples, n_classes)\n",
    "    Returns:\n",
    "      losses: list of loss per epoch\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for epoch in range(self.config.max_epochs):\n",
    "      start_time = time.time()\n",
    "      average_loss = self.run_epoch(sess, input_data, input_labels)\n",
    "      duration = time.time() - start_time\n",
    "      # Print status to stdout.\n",
    "      print('Epoch %d: loss = %.2f (%.3f sec)'\n",
    "             % (epoch, average_loss, duration))\n",
    "      losses.append(average_loss)\n",
    "    return losses\n",
    "\n",
    "  def __init__(self, config):\n",
    "    \"\"\"Initializes the model.\n",
    "\n",
    "    Args:\n",
    "      config: A model configuration object of type Config\n",
    "    \"\"\"\n",
    "    self.config = config\n",
    "    # Generate placeholders for the images and labels.\n",
    "    self.load_data()\n",
    "    self.add_placeholders()\n",
    "    self.pred = self.add_model(self.input_placeholder)\n",
    "    self.loss = self.add_loss_op(self.pred)\n",
    "    self.train_op = self.add_training_op(self.loss)\n",
    "  \n",
    "def test_SoftmaxModel():\n",
    "  \"\"\"Train softmax model for a number of steps.\"\"\"\n",
    "  config = Config()\n",
    "  with tf.Graph().as_default():\n",
    "    model = SoftmaxModel(config)\n",
    "  \n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "  \n",
    "    # Run the Op to initialize the variables.\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "  \n",
    "    losses = model.fit(sess, model.input_data, model.input_labels)\n",
    "\n",
    "  # If ops are implemented correctly, the average loss should fall close to zero\n",
    "  # rapidly.\n",
    "  assert losses[-1] < .5\n",
    "  print (\"Basic (non-exhaustive) classifier tests pass\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_SoftmaxModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q2_initialization.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile q2_initialization.py\n",
    "# %load q2_initialization.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def xavier_weight_init():\n",
    "  \"\"\"\n",
    "  Returns function that creates random tensor. \n",
    "\n",
    "  The specified function will take in a shape (tuple or 1-d array) and must\n",
    "  return a random tensor of the specified shape and must be drawn from the\n",
    "  Xavier initialization distribution.\n",
    "\n",
    "  Hint: You might find tf.random_uniform useful.\n",
    "  \"\"\"\n",
    "  def _xavier_initializer(shape, **kwargs):\n",
    "    \"\"\"Defines an initializer for the Xavier distribution.\n",
    "\n",
    "    This function will be used as a variable scope initializer.\n",
    "\n",
    "    https://www.tensorflow.org/versions/r0.7/how_tos/variable_scope/index.html#initializers-in-variable-scope\n",
    "\n",
    "    Args:\n",
    "      shape: Tuple or 1-d array that species dimensions of requested tensor.\n",
    "    Returns:\n",
    "      out: tf.Tensor of specified shape sampled from Xavier distribution.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    epsilon = np.sqrt(6.0 / np.sum(shape)) #this is a constant, so I can use numpy\n",
    "    out = tf.random_uniform(shape = shape, minval = -epsilon, maxval = epsilon)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return out\n",
    "  # Returns defined initializer function.\n",
    "  return _xavier_initializer\n",
    "\n",
    "def test_initialization_basic():\n",
    "  \"\"\"\n",
    "  Some simple tests for the initialization.\n",
    "  \"\"\"\n",
    "  print (\"Running basic tests...\")\n",
    "  xavier_initializer = xavier_weight_init()\n",
    "  shape = (1,)\n",
    "  xavier_mat = xavier_initializer(shape)\n",
    "  assert xavier_mat.get_shape() == shape\n",
    "\n",
    "  shape = (1, 2, 3)\n",
    "  xavier_mat = xavier_initializer(shape)\n",
    "  assert xavier_mat.get_shape() == shape\n",
    "  print (\"Basic (non-exhaustive) Xavier initialization tests pass\\n\")\n",
    "\n",
    "def test_initialization():\n",
    "  \"\"\" \n",
    "  Use this space to test your Xavier initialization code by running:\n",
    "      python q1_initialization.py \n",
    "  This function will not be called by the autograder, nor will\n",
    "  your tests be graded.\n",
    "  \"\"\"\n",
    "  print (\"Running your tests...\")\n",
    "  ### YOUR CODE HERE\n",
    "  raise NotImplementedError\n",
    "  ### END YOUR CODE  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_initialization_basic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training loss: 1.06377208233208233\n",
      "Training acc: 0.792428089441\n",
      "Validation loss: 1.01473462582\n",
      "\n",
      "[[42630    28    15    24    62]\n",
      " [  329  1548    38    93    86]\n",
      " [  515    75   481   164    33]\n",
      " [  977   233   111   522   249]\n",
      " [  972    24     2    32  2119]]\n",
      "Tag: O - P 0.9385 / R 0.9970\n",
      "Tag: LOC - P 0.8113 / R 0.7393\n",
      "Tag: MISC - P 0.7434 / R 0.3793\n",
      "Tag: ORG - P 0.6251 / R 0.2495\n",
      "Tag: PER - P 0.8313 / R 0.6729\n",
      "Total time: 192.207998037\n",
      "Epoch 1\n",
      "3097 / 3181 : loss = 1.00458061695"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c03aaf39e9e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m   \u001b[0mtest_NER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-c03aaf39e9e5>\u001b[0m in \u001b[0;36mtest_NER\u001b[0;34m()\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         train_loss, train_acc = model.run_epoch(session, model.X_train,\n\u001b[0;32m--> 401\u001b[0;31m                                                 model.y_train)\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'Training loss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c03aaf39e9e5>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, session, input_data, input_labels, shuffle, verbose)\u001b[0m\n\u001b[1;32m    307\u001b[0m       loss, total_correct, _ = session.run(\n\u001b[1;32m    308\u001b[0m           \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m           feed_dict=feed)\n\u001b[0m\u001b[1;32m    310\u001b[0m       \u001b[0mtotal_processed_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m       \u001b[0mtotal_correct_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/goodnotes/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %load q2_NER.py\n",
    "import os\n",
    "import getpass\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from q2_initialization import xavier_weight_init\n",
    "import data_utils.utils as du\n",
    "import data_utils.ner as ner\n",
    "from utils import data_iterator\n",
    "from model import LanguageModel\n",
    "\n",
    "class Config(object):\n",
    "  \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "  The config class is used to store various hyperparameters and dataset\n",
    "  information parameters. Model objects are passed a Config() object at\n",
    "  instantiation.\n",
    "  \"\"\"\n",
    "  embed_size = 50\n",
    "  batch_size = 64\n",
    "  label_size = 5\n",
    "  hidden_size = 100\n",
    "  max_epochs = 24 \n",
    "  early_stopping = 2\n",
    "  dropout = 0.9\n",
    "  lr = 0.001\n",
    "  l2 = 0.001\n",
    "  window_size = 3\n",
    "\n",
    "class NERModel(LanguageModel):\n",
    "  \"\"\"Implements a NER (Named Entity Recognition) model.\n",
    "\n",
    "  This class implements a deep network for named entity recognition. It\n",
    "  inherits from LanguageModel, which has an add_embedding method in addition to\n",
    "  the standard Model method.\n",
    "  \"\"\"\n",
    "\n",
    "  def load_data(self, debug=False):\n",
    "    \"\"\"Loads starter word-vectors and train/dev/test data.\"\"\"\n",
    "    # Load the starter word vectors\n",
    "    self.wv, word_to_num, num_to_word = ner.load_wv(\n",
    "      'data/ner/vocab.txt', 'data/ner/wordVectors.txt')\n",
    "    tagnames = ['O', 'LOC', 'MISC', 'ORG', 'PER']\n",
    "    self.num_to_tag = dict(enumerate(tagnames))\n",
    "    tag_to_num = {v:k for k,v in self.num_to_tag.iteritems()}\n",
    "\n",
    "    # Load the training set\n",
    "    docs = du.load_dataset('data/ner/train')\n",
    "    self.X_train, self.y_train = du.docs_to_windows(\n",
    "        docs, word_to_num, tag_to_num, wsize=self.config.window_size)\n",
    "    if debug:\n",
    "      self.X_train = self.X_train[:1024]\n",
    "      self.y_train = self.y_train[:1024]\n",
    "\n",
    "    # Load the dev set (for tuning hyperparameters)\n",
    "    docs = du.load_dataset('data/ner/dev')\n",
    "    self.X_dev, self.y_dev = du.docs_to_windows(\n",
    "        docs, word_to_num, tag_to_num, wsize=self.config.window_size)\n",
    "    if debug:\n",
    "      self.X_dev = self.X_dev[:1024]\n",
    "      self.y_dev = self.y_dev[:1024]\n",
    "\n",
    "    # Load the test set (dummy labels only)\n",
    "    docs = du.load_dataset('data/ner/test.masked')\n",
    "    self.X_test, self.y_test = du.docs_to_windows(\n",
    "        docs, word_to_num, tag_to_num, wsize=self.config.window_size)\n",
    "\n",
    "  def add_placeholders(self):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors\n",
    "\n",
    "    These placeholders are used as inputs by the rest of the model building\n",
    "    code and will be fed data during training.  Note that when \"None\" is in a\n",
    "    placeholder's shape, it's flexible\n",
    "\n",
    "    Adds following nodes to the computational graph\n",
    "\n",
    "    input_placeholder: Input placeholder tensor of shape\n",
    "                       (None, window_size), type tf.int32\n",
    "    labels_placeholder: Labels placeholder tensor of shape\n",
    "                        (None, label_size), type tf.float32\n",
    "    dropout_placeholder: Dropout value placeholder (scalar),\n",
    "                         type tf.float32\n",
    "\n",
    "    Add these placeholders to self as the instance variables\n",
    "  \n",
    "      self.input_placeholder\n",
    "      self.labels_placeholder\n",
    "      self.dropout_placeholder\n",
    "\n",
    "    (Don't change the variable names)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    self.input_placeholder = tf.placeholder(dtype=tf.int32, shape=(None, self.config.window_size))\n",
    "    self.labels_placeholder = tf.placeholder(dtype=tf.float32, shape=(None, self.config.label_size))\n",
    "    self.dropout_placeholder = tf.placeholder(dtype=tf.float32, shape=()) #shape = () means scalar\n",
    "    ### END YOUR CODE\n",
    "\n",
    "  def create_feed_dict(self, input_batch, dropout, label_batch=None):\n",
    "    \"\"\"Creates the feed_dict for softmax classifier.\n",
    "\n",
    "    A feed_dict takes the form of:\n",
    "\n",
    "    feed_dict = {\n",
    "        <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "        ....\n",
    "    }\n",
    "\n",
    "\n",
    "    Hint: The keys for the feed_dict should be a subset of the placeholder\n",
    "          tensors created in add_placeholders.\n",
    "    Hint: When label_batch is None, don't add a labels entry to the feed_dict.\n",
    "    \n",
    "    Args:\n",
    "      input_batch: A batch of input data.\n",
    "      label_batch: A batch of label data.\n",
    "    Returns:\n",
    "      feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    feed_dict = {\n",
    "        self.input_placeholder:input_batch,\n",
    "        self.dropout_placeholder:dropout,\n",
    "    }\n",
    "    if label_batch is not None:\n",
    "      feed_dict[self.labels_placeholder] = label_batch\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    return feed_dict\n",
    "\n",
    "  def add_embedding(self):\n",
    "    \"\"\"Add embedding layer that maps from vocabulary to vectors.\n",
    "\n",
    "    Creates an embedding tensor (of shape (len(self.wv), embed_size). Use the\n",
    "    input_placeholder to retrieve the embeddings for words in the current batch.\n",
    "\n",
    "    (Words are discrete entities. They need to be transformed into vectors for use\n",
    "    in deep-learning. Although we won't do so in this problem, in practice it's\n",
    "    useful to initialize the embedding with pre-trained word-vectors. For this\n",
    "    problem, using the default initializer is sufficient.)\n",
    "\n",
    "    Hint: This layer should use the input_placeholder to index into the\n",
    "          embedding.\n",
    "    Hint: You might find tf.nn.embedding_lookup useful.\n",
    "    Hint: See following link to understand what -1 in a shape means.\n",
    "      https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#reshape\n",
    "    Hint: Check the last slide from the TensorFlow lecture.\n",
    "    Hint: Here are the dimensions of the variables you will need to create:\n",
    "\n",
    "      L: (len(self.wv), embed_size)\n",
    "\n",
    "    Returns:\n",
    "      window: tf.Tensor of shape (-1, window_size*embed_size)\n",
    "    \"\"\"\n",
    "    # The embedding lookup is currently only implemented for the CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "      ### YOUR CODE HERE\n",
    "        with tf.variable_scope(\"embedding_layer\") as scope:\n",
    "            # Create variable named \"embedding\", we can reuse this variable with this name later if we want.\n",
    "            # This is the embedding vector table, or the lookup table\n",
    "            embedding = tf.get_variable(\"embedding\",shape=[len(self.wv), self.config.embed_size],\n",
    "                                        initializer=xavier_weight_init())\n",
    "            \n",
    "            # params = A single tensor representing the complete embedding tensor, which means the lookup table\n",
    "            # ids is a Tensor with type int32 or int64 containing the ids to be looked up in params which is the input.\n",
    "            # returned value is the embedding word vector.\n",
    "            window = tf.nn.embedding_lookup(params=embedding, ids=self.input_placeholder)\n",
    "            \n",
    "            window = tf.reshape(window, shape=[-1, self.config.window_size * self.config.embed_size], name=\"window\")\n",
    "            # variable_summaries(window, window.name) <-- for tensorboard?\n",
    "            \n",
    "      ### END YOUR CODE\n",
    "    return window\n",
    "\n",
    "  def add_model(self, window):\n",
    "    \"\"\"Adds the 1-hidden-layer NN.\n",
    "\n",
    "    Hint: Use a variable_scope (e.g. \"Layer\") for the first hidden layer, and\n",
    "          another variable_scope (e.g. \"Softmax\") for the linear transformation\n",
    "          preceding the softmax. Make sure to use the xavier_weight_init you\n",
    "          defined in the previous part to initialize weights.\n",
    "    Hint: Make sure to add in regularization and dropout to this network.\n",
    "          Regularization should be an addition to the cost function, while\n",
    "          dropout should be added after both variable scopes.\n",
    "    Hint: You might consider using a tensorflow Graph Collection (e.g\n",
    "          \"total_loss\") to collect the regularization and loss terms (which you\n",
    "          will add in add_loss_op below).\n",
    "    Hint: Here are the dimensions of the various variables you will need to\n",
    "          create\n",
    "\n",
    "          W:  (window_size*embed_size, hidden_size)\n",
    "          b1: (hidden_size,)\n",
    "          U:  (hidden_size, label_size)\n",
    "          b2: (label_size)\n",
    "\n",
    "    https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#graph-collections\n",
    "    Args:\n",
    "      window: tf.Tensor of shape (-1, window_size*embed_size)\n",
    "    Returns:\n",
    "      output: tf.Tensor of shape (batch_size, label_size)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    with tf.variable_scope(\"Layer\") as layer_scope:\n",
    "        W = tf.get_variable(shape=(self.config.window_size*self.config.embed_size, self.config.hidden_size),\n",
    "                        initializer=xavier_weight_init(), name=\"Weight\")\n",
    "        b1 = tf.get_variable(shape=(self.config.hidden_size),\n",
    "                        initializer=tf.constant_initializer(0.0), name = \"Bias\")\n",
    "\n",
    "    with tf.variable_scope(\"Softmax\") as hidden_layer:\n",
    "        U = tf.get_variable(shape=(self.config.hidden_size, self.config.label_size),\n",
    "                        initializer=xavier_weight_init(), name=\"Weight\")\n",
    "        b2 = tf.get_variable(shape=(self.config.label_size),\n",
    "                        initializer=tf.constant_initializer(0.0), name = \"Bias\")\n",
    "\n",
    "    # forward propagation    \n",
    "    Z1 = tf.matmul(window,W)+b1\n",
    "    h = tf.tanh(Z1)\n",
    "#     h = tf.nn.dropout(h, self.dropout_placeholder)\n",
    "    \n",
    "    Z2 = tf.matmul(h, U)+b2\n",
    "    yhat = tf.nn.softmax(Z2)\n",
    "    yhat = tf.nn.dropout(yhat, self.dropout_placeholder)\n",
    "    \n",
    "    # we also need to calculate L2 loss here for regularization.\n",
    "#     l2_loss = tf.nn.l2_loss(W) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(U) + tf.nn.l2_loss(b2)\n",
    "    l2_loss = tf.nn.l2_loss(W) + tf.nn.l2_loss(U)      ###################\n",
    "    tf.add_to_collection(name=\"l2_loss\", value=l2_loss)\n",
    "\n",
    "    output = yhat\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return output \n",
    "\n",
    "  def add_loss_op(self, y):\n",
    "    \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "    Hint: You can use tf.nn.softmax_cross_entropy_with_logits to simplify your\n",
    "          implementation. You might find tf.reduce_mean useful.\n",
    "    Args:\n",
    "      pred: A tensor of shape (batch_size, n_classes)\n",
    "    Returns:\n",
    "      loss: A 0-d tensor (scalar)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=self.labels_placeholder))\n",
    "    \n",
    "    # Regularization\n",
    "    loss += self.config.l2*tf.get_collection(\"l2_loss\")[0] #############\n",
    "    ### END YOUR CODE\n",
    "    return loss\n",
    "\n",
    "  def add_training_op(self, loss):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "\n",
    "    Creates an optimizer and applies the gradients to all trainable variables.\n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train. See \n",
    "\n",
    "    https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "    for more information.\n",
    "\n",
    "    Hint: Use tf.train.AdamOptimizer for this model.\n",
    "          Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "    Args:\n",
    "      loss: Loss tensor, from cross_entropy_loss.\n",
    "    Returns:\n",
    "      train_op: The Op for training.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    train_op = tf.train.AdamOptimizer(self.config.lr).minimize(loss=loss)\n",
    "    ### END YOUR CODE\n",
    "    return train_op\n",
    "\n",
    "  def __init__(self, config):\n",
    "    \"\"\"Constructs the network using the helper functions defined above.\"\"\"\n",
    "    self.config = config\n",
    "    self.load_data(debug=False)\n",
    "    self.add_placeholders()\n",
    "    window = self.add_embedding()\n",
    "    y = self.add_model(window)\n",
    "\n",
    "    self.loss = self.add_loss_op(y)\n",
    "    self.predictions = tf.nn.softmax(y)\n",
    "    one_hot_prediction = tf.argmax(self.predictions, 1)\n",
    "    correct_prediction = tf.equal(\n",
    "        tf.argmax(self.labels_placeholder, 1), one_hot_prediction)\n",
    "    self.correct_predictions = tf.reduce_sum(tf.cast(correct_prediction, 'int32'))\n",
    "    self.train_op = self.add_training_op(self.loss)\n",
    "\n",
    "  def run_epoch(self, session, input_data, input_labels,\n",
    "                shuffle=True, verbose=True):\n",
    "    orig_X, orig_y = input_data, input_labels\n",
    "    dp = self.config.dropout\n",
    "    # We're interested in keeping track of the loss and accuracy during training\n",
    "    total_loss = []\n",
    "    total_correct_examples = 0\n",
    "    total_processed_examples = 0\n",
    "    total_steps = len(orig_X) / self.config.batch_size\n",
    "    for step, (x, y) in enumerate(\n",
    "      data_iterator(orig_X, orig_y, batch_size=self.config.batch_size,\n",
    "                   label_size=self.config.label_size, shuffle=shuffle)):\n",
    "      feed = self.create_feed_dict(input_batch=x, dropout=dp, label_batch=y)\n",
    "      loss, total_correct, _ = session.run(\n",
    "          [self.loss, self.correct_predictions, self.train_op],\n",
    "          feed_dict=feed)\n",
    "      total_processed_examples += len(x)\n",
    "      total_correct_examples += total_correct\n",
    "      total_loss.append(loss)\n",
    "      ##\n",
    "      if verbose and step % verbose == 0:\n",
    "        sys.stdout.write('\\r{} / {} : loss = {}'.format(\n",
    "            step, total_steps, np.mean(total_loss)))\n",
    "        sys.stdout.flush()\n",
    "    if verbose:\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.flush()\n",
    "    return np.mean(total_loss), total_correct_examples / float(total_processed_examples)\n",
    "\n",
    "  def predict(self, session, X, y=None):\n",
    "    \"\"\"Make predictions from the provided model.\"\"\"\n",
    "    # If y is given, the loss is also calculated\n",
    "    # We deactivate dropout by setting it to 1\n",
    "    dp = 1\n",
    "    losses = []\n",
    "    results = []\n",
    "    if np.any(y):\n",
    "        data = data_iterator(X, y, batch_size=self.config.batch_size,\n",
    "                             label_size=self.config.label_size, shuffle=False)\n",
    "    else:\n",
    "        data = data_iterator(X, batch_size=self.config.batch_size,\n",
    "                             label_size=self.config.label_size, shuffle=False)\n",
    "    for step, (x, y) in enumerate(data):\n",
    "      feed = self.create_feed_dict(input_batch=x, dropout=dp)\n",
    "      if np.any(y):\n",
    "        feed[self.labels_placeholder] = y\n",
    "        loss, preds = session.run(\n",
    "            [self.loss, self.predictions], feed_dict=feed)\n",
    "        losses.append(loss)\n",
    "      else:\n",
    "        preds = session.run(self.predictions, feed_dict=feed)\n",
    "      predicted_indices = preds.argmax(axis=1)\n",
    "      results.extend(predicted_indices)\n",
    "    return np.mean(losses), results\n",
    "\n",
    "def print_confusion(confusion, num_to_tag):\n",
    "    \"\"\"Helper method that prints confusion matrix.\"\"\"\n",
    "    # Summing top to bottom gets the total number of tags guessed as T\n",
    "    total_guessed_tags = confusion.sum(axis=0)\n",
    "    # Summing left to right gets the total number of true tags\n",
    "    total_true_tags = confusion.sum(axis=1)\n",
    "    print\n",
    "    print confusion\n",
    "    for i, tag in sorted(num_to_tag.items()):\n",
    "        prec = confusion[i, i] / float(total_guessed_tags[i])\n",
    "        recall = confusion[i, i] / float(total_true_tags[i])\n",
    "        print 'Tag: {} - P {:2.4f} / R {:2.4f}'.format(tag, prec, recall)\n",
    "\n",
    "def calculate_confusion(config, predicted_indices, y_indices):\n",
    "    \"\"\"Helper method that calculates confusion matrix.\"\"\"\n",
    "    confusion = np.zeros((config.label_size, config.label_size), dtype=np.int32)\n",
    "    for i in xrange(len(y_indices)):\n",
    "        correct_label = y_indices[i]\n",
    "        guessed_label = predicted_indices[i]\n",
    "        confusion[correct_label, guessed_label] += 1\n",
    "    return confusion\n",
    "\n",
    "def save_predictions(predictions, filename):\n",
    "  \"\"\"Saves predictions to provided file.\"\"\"\n",
    "  with open(filename, \"wb\") as f:\n",
    "    for prediction in predictions:\n",
    "      f.write(str(prediction) + \"\\n\")\n",
    "\n",
    "def test_NER():\n",
    "  \"\"\"Test NER model implementation.\n",
    "\n",
    "  You can use this function to test your implementation of the Named Entity\n",
    "  Recognition network. When debugging, set max_epochs in the Config object to 1\n",
    "  so you can rapidly iterate.\n",
    "  \"\"\"\n",
    "  config = Config()\n",
    "  with tf.Graph().as_default():\n",
    "    model = NERModel(config)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as session:\n",
    "      best_val_loss = float('inf')\n",
    "      best_val_epoch = 0\n",
    "\n",
    "      session.run(init)\n",
    "      for epoch in xrange(config.max_epochs):\n",
    "        print 'Epoch {}'.format(epoch)\n",
    "        start = time.time()\n",
    "        ###\n",
    "        train_loss, train_acc = model.run_epoch(session, model.X_train,\n",
    "                                                model.y_train)\n",
    "        val_loss, predictions = model.predict(session, model.X_dev, model.y_dev)\n",
    "        print 'Training loss: {}'.format(train_loss)\n",
    "        print 'Training acc: {}'.format(train_acc)\n",
    "        print 'Validation loss: {}'.format(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "          best_val_loss = val_loss\n",
    "          best_val_epoch = epoch\n",
    "          if not os.path.exists(\"./weights\"):\n",
    "            os.makedirs(\"./weights\")\n",
    "        \n",
    "          saver.save(session, './weights/ner.weights')\n",
    "        if epoch - best_val_epoch > config.early_stopping:\n",
    "          break\n",
    "        ###\n",
    "        confusion = calculate_confusion(config, predictions, model.y_dev)\n",
    "        print_confusion(confusion, model.num_to_tag)\n",
    "        print 'Total time: {}'.format(time.time() - start)\n",
    "      \n",
    "      saver.restore(session, './weights/ner.weights')\n",
    "      print 'Test'\n",
    "      print '=-=-='\n",
    "      print 'Writing predictions to q2_test.predicted'\n",
    "      _, predictions = model.predict(session, model.X_test, model.y_test)\n",
    "      save_predictions(predictions, \"q2_test.predicted\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  test_NER()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load q3_RNNLM.py\n",
    "import getpass\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils import calculate_perplexity, get_ptb_dataset, Vocab\n",
    "from utils import ptb_iterator, sample\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.seq2seq import sequence_loss\n",
    "from model import LanguageModel\n",
    "\n",
    "# Let's set the parameters of our model\n",
    "# http://arxiv.org/pdf/1409.2329v4.pdf shows parameters that would achieve near\n",
    "# SotA numbers\n",
    "\n",
    "class Config(object):\n",
    "  \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "  The config class is used to store various hyperparameters and dataset\n",
    "  information parameters. Model objects are passed a Config() object at\n",
    "  instantiation.\n",
    "  \"\"\"\n",
    "  batch_size = 64\n",
    "  embed_size = 50\n",
    "  hidden_size = 100\n",
    "  num_steps = 10\n",
    "  max_epochs = 16\n",
    "  early_stopping = 2\n",
    "  dropout = 0.9\n",
    "  lr = 0.001\n",
    "\n",
    "class RNNLM_Model(LanguageModel):\n",
    "\n",
    "  def load_data(self, debug=False):\n",
    "    \"\"\"Loads starter word-vectors and train/dev/test data.\"\"\"\n",
    "    self.vocab = Vocab()\n",
    "    self.vocab.construct(get_ptb_dataset('train'))\n",
    "    self.encoded_train = np.array(\n",
    "        [self.vocab.encode(word) for word in get_ptb_dataset('train')],\n",
    "        dtype=np.int32)\n",
    "    self.encoded_valid = np.array(\n",
    "        [self.vocab.encode(word) for word in get_ptb_dataset('valid')],\n",
    "        dtype=np.int32)\n",
    "    self.encoded_test = np.array(\n",
    "        [self.vocab.encode(word) for word in get_ptb_dataset('test')],\n",
    "        dtype=np.int32)\n",
    "    if debug:\n",
    "      num_debug = 1024\n",
    "      self.encoded_train = self.encoded_train[:num_debug]\n",
    "      self.encoded_valid = self.encoded_valid[:num_debug]\n",
    "      self.encoded_test = self.encoded_test[:num_debug]\n",
    "\n",
    "  def add_placeholders(self):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors\n",
    "\n",
    "    These placeholders are used as inputs by the rest of the model building\n",
    "    code and will be fed data during training.  Note that when \"None\" is in a\n",
    "    placeholder's shape, it's flexible\n",
    "\n",
    "    Adds following nodes to the computational graph.\n",
    "    (When None is in a placeholder's shape, it's flexible)\n",
    "\n",
    "    input_placeholder: Input placeholder tensor of shape\n",
    "                       (None, num_steps), type tf.int32\n",
    "    labels_placeholder: Labels placeholder tensor of shape\n",
    "                        (None, num_steps), type tf.float32\n",
    "    dropout_placeholder: Dropout value placeholder (scalar),\n",
    "                         type tf.float32\n",
    "\n",
    "    Add these placeholders to self as the instance variables\n",
    "  \n",
    "      self.input_placeholder\n",
    "      self.labels_placeholder\n",
    "      self.dropout_placeholder\n",
    "\n",
    "    (Don't change the variable names)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    self.input_placeholder = tf.placeholder(dtype=tf.int32, shape=(None,self.config.num_steps))\n",
    "    self.labels_placeholder = tf.placeholder(dtype=tf.float32, shape=(None,self.config.num_steps))\n",
    "    self.dropout_placeholder = tf.placeholder(dtype=tf.float32,shape=())\n",
    "    ### END YOUR CODE\n",
    "  \n",
    "  def add_embedding(self):\n",
    "    \"\"\"Add embedding layer.\n",
    "\n",
    "    Hint: This layer should use the input_placeholder to index into the\n",
    "          embedding.\n",
    "    Hint: You might find tf.nn.embedding_lookup useful.\n",
    "    Hint: You might find tf.split, tf.squeeze useful in constructing tensor inputs\n",
    "    Hint: Check the last slide from the TensorFlow lecture.\n",
    "    Hint: Here are the dimensions of the variables you will need to create:\n",
    "\n",
    "      L: (len(self.vocab), embed_size)\n",
    "\n",
    "    Returns:\n",
    "      inputs: List of length num_steps, each of whose elements should be\n",
    "              a tensor of shape (batch_size, embed_size).\n",
    "    \"\"\"\n",
    "    # The embedding lookup is currently only implemented for the CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        ### YOUR CODE HERE\n",
    "        L = tf.get_variable(dtype=tf.float32,shape=(len(self.vocab),self.config.embed_size), \n",
    "                          initializer=tf.random_uniform_initializer(minval=-1,maxval=1))\n",
    "        \n",
    "        inputs = tf.nn.embedding_lookup(params=L,ids=self.input_placeholder)\n",
    "        print(\"input before split shape =\",input.get_shape())\n",
    "\n",
    "        #split the input into shape (?, self.config.num_steps, self.config.embed_size)\n",
    "        inputs = tf.split(value=inputs,num_or_size_splits=self.config.num_steps,axis=1) \n",
    "        \n",
    "#         print(\"input after split shape =\",input.get_shape())\n",
    "        \n",
    "        #squeeze removes all size1 dimensions at dim=1.\n",
    "        for i in range(len(inputs)):\n",
    "            inputs[i] = tf.squeeze(inputs[i],axis=1)\n",
    "            \n",
    "        print(\"input after squeeze shape =\",input.get_shape())\n",
    "\n",
    "        ### END YOUR CODE\n",
    "      return inputs\n",
    "\n",
    "  def add_projection(self, rnn_outputs):\n",
    "    \"\"\"Adds a projection layer.\n",
    "\n",
    "    The projection layer transforms the hidden representation to a distribution\n",
    "    over the vocabulary.\n",
    "\n",
    "    Hint: Here are the dimensions of the variables you will need to\n",
    "          create \n",
    "          \n",
    "          U:   (hidden_size, len(vocab))\n",
    "          b_2: (len(vocab),)\n",
    "\n",
    "    Args:\n",
    "      rnn_outputs: List of length num_steps, each of whose elements should be\n",
    "                   a tensor of shape (batch_size, embed_size).\n",
    "    Returns:\n",
    "      outputs: List of length num_steps, each a tensor of shape\n",
    "               (batch_size, len(vocab)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    with tf.variable_scope(\"projection\"):\n",
    "        U = tf.get_variable(shape=(self.config.hidden_size, len(self.vocab)),\n",
    "                           initializer=tf.random_uniform_initializer(minval=-1.0,maxval=1.0))\n",
    "        b2 = tf.get_variable(shape=(,len(self.vocab)), \n",
    "                             initializer=tf.random_uniform_initializer(minval=-1.0,maxval=1.0))\n",
    "    \n",
    "    # loop the num_steps times, output is a list of tensors with shape(batch_size,len(vocab))\n",
    "    outputs = []\n",
    "    \n",
    "    for rnn_step in rnn_outputs:\n",
    "        temp = tf.matmul(rnn_step,U)+b2\n",
    "        outputs.append(out)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    return outputs\n",
    "\n",
    "  def add_loss_op(self, output):\n",
    "    \"\"\"Adds loss ops to the computational graph.\n",
    "\n",
    "    Hint: Use tensorflow.python.ops.seq2seq.sequence_loss to implement sequence loss. \n",
    "\n",
    "    Args:\n",
    "      output: A tensor of shape (None, self.vocab)\n",
    "    Returns:\n",
    "      loss: A 0-d tensor (scalar)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    # this all_ones_weight is needed for using sequence_loss.\n",
    "    all_ones_weight = [tf.ones(shape=(self.config.batch_size*self.config.num_steps))]\n",
    "    CE_loss = tf.contrib.seq2seq.sequence_loss(logits=[output],targets=self.labels_placeholders,\n",
    "                                           weights=all_ones_weight)\n",
    "    \n",
    "    # We need to add ALL the total_loss, so we have to add it to collection and sum them up later.\n",
    "    tf.add_to_collection(\"total_loss\", CE_loss)\n",
    "    loss=tf.add_n(tf.get_collection('total_loss'))\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    return loss\n",
    "\n",
    "  def add_training_op(self, loss):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "\n",
    "    Creates an optimizer and applies the gradients to all trainable variables.\n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train. See \n",
    "\n",
    "    https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "    for more information.\n",
    "\n",
    "    Hint: Use tf.train.AdamOptimizer for this model.\n",
    "          Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "    Args:\n",
    "      loss: Loss tensor, from cross_entropy_loss.\n",
    "    Returns:\n",
    "      train_op: The Op for training.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    train_op = tf.train.AdamOptimizer(self.config.lr).minimize(loss)\n",
    "    ### END YOUR CODE\n",
    "    return train_op\n",
    "  \n",
    "  def __init__(self, config):\n",
    "    self.config = config\n",
    "    self.load_data(debug=False)\n",
    "    self.add_placeholders()\n",
    "    self.inputs = self.add_embedding()\n",
    "    self.rnn_outputs = self.add_model(self.inputs)\n",
    "    self.outputs = self.add_projection(self.rnn_outputs)\n",
    "  \n",
    "    # We want to check how well we correctly predict the next word\n",
    "    # We cast o to float64 as there are numerical issues at hand\n",
    "    # (i.e. sum(output of softmax) = 1.00000298179 and not 1)\n",
    "    self.predictions = [tf.nn.softmax(tf.cast(o, 'float64')) for o in self.outputs]\n",
    "    # Reshape the output into len(vocab) sized chunks - the -1 says as many as\n",
    "    # needed to evenly divide\n",
    "    output = tf.reshape(tf.concat(1, self.outputs), [-1, len(self.vocab)])\n",
    "    self.calculate_loss = self.add_loss_op(output)\n",
    "    self.train_step = self.add_training_op(self.calculate_loss)\n",
    "\n",
    "\n",
    "  def add_model(self, inputs):\n",
    "    \"\"\"Creates the RNN LM model.\n",
    "\n",
    "    In the space provided below, you need to implement the equations for the\n",
    "    RNN LM model. Note that you may NOT use built in rnn_cell functions from\n",
    "    tensorflow.\n",
    "\n",
    "    Hint: Use a zeros tensor of shape (batch_size, hidden_size) as\n",
    "          initial state for the RNN. Add this to self as instance variable\n",
    "\n",
    "          self.initial_state\n",
    "  \n",
    "          (Don't change variable name)\n",
    "    Hint: Add the last RNN output to self as instance variable\n",
    "\n",
    "          self.final_state\n",
    "\n",
    "          (Don't change variable name)\n",
    "    Hint: Make sure to apply dropout to the inputs and the outputs.\n",
    "    Hint: Use a variable scope (e.g. \"RNN\") to define RNN variables.\n",
    "    Hint: Perform an explicit for-loop over inputs. You can use\n",
    "          scope.reuse_variables() to ensure that the weights used at each\n",
    "          iteration (each time-step) are the same. (Make sure you don't call\n",
    "          this for iteration 0 though or nothing will be initialized!)\n",
    "    Hint: Here are the dimensions of the various variables you will need to\n",
    "          create:\n",
    "      \n",
    "          H: (hidden_size, hidden_size) \n",
    "          I: (embed_size, hidden_size)\n",
    "          b_1: (hidden_size,)\n",
    "\n",
    "    Args:\n",
    "      inputs: List of length num_steps, each of whose elements should be\n",
    "              a tensor of shape (batch_size, embed_size).\n",
    "    Returns:\n",
    "      outputs: List of length num_steps, each of whose elements should be\n",
    "               a tensor of shape (batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    with tf.variable_scope(\"InputDropout\"):\n",
    "        inputs = [tf.nn.dropout(i, self.dropout_placeholder) for i in inputs]\n",
    "    \n",
    "    with tf.variable_scope(\"rnn\") as scope:\n",
    "        # set initial_state to be a zeros tensor\n",
    "        self.initial_state = tf.zeros(shape=(self.config.batch_size,self.config.hidden_size))\n",
    "        \n",
    "        # set a temp variable for looping state, and set output to empty\n",
    "        state = self.initial_state\n",
    "        rnn_outputs = []\n",
    "        \n",
    "        for step, current_input in enumerate(inputs):\n",
    "            if step > 0:\n",
    "                scope.reuse_variables()\n",
    "                \n",
    "            H = tf.get_variable(shape=(self.config.hidden_size,self.config.hidden_size),\n",
    "                               initializer=tf.random_uniform_initializer(minval=-1.0,maxval=1.0))\n",
    "            I = tf.get_variable(shape=(self.config.embed_size,self.config.hidden_size),\n",
    "                               initializer=tf.random_uniform_initializer(minval=-1.0,maxval=1.0))\n",
    "            b1 = tf.get_variable(shape=(self.config.hidden_size,),\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "            Z1 =  tf.matmul(state, H) + tf.matmul(inputs,I) + b1\n",
    "            state = tf.sigmoid(Z1)\n",
    "            rnn_outputs.append(state)\n",
    "            \n",
    "    self.final_state = rnn_outputs[-1] #this picks the final one, which is the final state        \n",
    "    ### END YOUR CODE\n",
    "    return rnn_outputs\n",
    "\n",
    "\n",
    "  def run_epoch(self, session, data, train_op=None, verbose=10):\n",
    "    config = self.config\n",
    "    dp = config.dropout\n",
    "    if not train_op:\n",
    "      train_op = tf.no_op()\n",
    "      dp = 1\n",
    "    total_steps = sum(1 for x in ptb_iterator(data, config.batch_size, config.num_steps))\n",
    "    total_loss = []\n",
    "    state = self.initial_state.eval()\n",
    "    for step, (x, y) in enumerate(\n",
    "      ptb_iterator(data, config.batch_size, config.num_steps)):\n",
    "      # We need to pass in the initial state and retrieve the final state to give\n",
    "      # the RNN proper history\n",
    "      feed = {self.input_placeholder: x,\n",
    "              self.labels_placeholder: y,\n",
    "              self.initial_state: state,\n",
    "              self.dropout_placeholder: dp}\n",
    "      loss, state, _ = session.run(\n",
    "          [self.calculate_loss, self.final_state, train_op], feed_dict=feed)\n",
    "      total_loss.append(loss)\n",
    "      if verbose and step % verbose == 0:\n",
    "          sys.stdout.write('\\r{} / {} : pp = {}'.format(\n",
    "              step, total_steps, np.exp(np.mean(total_loss))))\n",
    "          sys.stdout.flush()\n",
    "    if verbose:\n",
    "      sys.stdout.write('\\r')\n",
    "    return np.exp(np.mean(total_loss))\n",
    "\n",
    "def generate_text(session, model, config, starting_text='<eos>',\n",
    "                  stop_length=100, stop_tokens=None, temp=1.0):\n",
    "  \"\"\"Generate text from the model.\n",
    "\n",
    "  Hint: Create a feed-dictionary and use sess.run() to execute the model. Note\n",
    "        that you will need to use model.initial_state as a key to feed_dict\n",
    "  Hint: Fetch model.final_state and model.predictions[-1]. (You set\n",
    "        model.final_state in add_model() and model.predictions is set in\n",
    "        __init__)\n",
    "  Hint: Store the outputs of running the model in local variables state and\n",
    "        y_pred (used in the pre-implemented parts of this function.)\n",
    "\n",
    "  Args:\n",
    "    session: tf.Session() object\n",
    "    model: Object of type RNNLM_Model\n",
    "    config: A Config() object\n",
    "    starting_text: Initial text passed to model.\n",
    "  Returns:\n",
    "    output: List of word idxs\n",
    "  \"\"\"\n",
    "  state = model.initial_state.eval()\n",
    "  # Imagine tokens as a batch size of one, length of len(tokens[0])\n",
    "  tokens = [model.vocab.encode(word) for word in starting_text.split()]\n",
    "  for i in xrange(stop_length):\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "    next_word_idx = sample(y_pred[0], temperature=temp)\n",
    "    tokens.append(next_word_idx)\n",
    "    if stop_tokens and model.vocab.decode(tokens[-1]) in stop_tokens:\n",
    "      break\n",
    "  output = [model.vocab.decode(word_idx) for word_idx in tokens]\n",
    "  return output\n",
    "\n",
    "def generate_sentence(session, model, config, *args, **kwargs):\n",
    "  \"\"\"Convenice to generate a sentence from the model.\"\"\"\n",
    "  return generate_text(session, model, config, *args, stop_tokens=['<eos>'], **kwargs)\n",
    "\n",
    "def test_RNNLM():\n",
    "  config = Config()\n",
    "  gen_config = deepcopy(config)\n",
    "  gen_config.batch_size = gen_config.num_steps = 1\n",
    "\n",
    "  # We create the training model and generative model\n",
    "  with tf.variable_scope('RNNLM') as scope:\n",
    "    model = RNNLM_Model(config)\n",
    "    # This instructs gen_model to reuse the same variables as the model above\n",
    "    scope.reuse_variables()\n",
    "    gen_model = RNNLM_Model(gen_config)\n",
    "\n",
    "  init = tf.initialize_all_variables()\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  with tf.Session() as session:\n",
    "    best_val_pp = float('inf')\n",
    "    best_val_epoch = 0\n",
    "  \n",
    "    session.run(init)\n",
    "    for epoch in xrange(config.max_epochs):\n",
    "      print ('Epoch {}'.format(epoch))\n",
    "      start = time.time()\n",
    "      ###\n",
    "      train_pp = model.run_epoch(\n",
    "          session, model.encoded_train,\n",
    "          train_op=model.train_step)\n",
    "      valid_pp = model.run_epoch(session, model.encoded_valid)\n",
    "      print 'Training perplexity: {}'.format(train_pp)\n",
    "      print 'Validation perplexity: {}'.format(valid_pp)\n",
    "      if valid_pp < best_val_pp:\n",
    "        best_val_pp = valid_pp\n",
    "        best_val_epoch = epoch\n",
    "        saver.save(session, './ptb_rnnlm.weights')\n",
    "      if epoch - best_val_epoch > config.early_stopping:\n",
    "        break\n",
    "      print 'Total time: {}'.format(time.time() - start)\n",
    "      \n",
    "    saver.restore(session, 'ptb_rnnlm.weights')\n",
    "    test_pp = model.run_epoch(session, model.encoded_test)\n",
    "    print '=-=' * 5\n",
    "    print 'Test perplexity: {}'.format(test_pp)\n",
    "    print '=-=' * 5\n",
    "    starting_text = 'in palo alto'\n",
    "    while starting_text:\n",
    "      print ' '.join(generate_sentence(\n",
    "          session, gen_model, gen_config, starting_text=starting_text, temp=1.0))\n",
    "      starting_text = raw_input('> ')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_RNNLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q3_RNNLM.py\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
